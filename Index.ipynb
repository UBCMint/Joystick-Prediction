{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akirakudo901/Joystick-Prediction/blob/feature_extraction/Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJhkLkR-lfsU"
      },
      "source": [
        "# ECoG Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5rTOOUGlfsX"
      },
      "source": [
        "## Downloading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE0Mu4AglfsY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import requests\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import lightgbm as lgb\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import rcParams\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import signal, fft\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvlpErMglfsa"
      },
      "outputs": [],
      "source": [
        "fname = 'joystick_track.npz'\n",
        "url = \"https://osf.io/6jncm/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9DFaNhFlfsa"
      },
      "outputs": [],
      "source": [
        "rcParams['figure.figsize'] = [20, 4]\n",
        "rcParams['font.size'] = 15\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False\n",
        "rcParams['figure.autolayout'] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scwrCWHTlfsb"
      },
      "source": [
        "## Dataset info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VMeByOXlfsb"
      },
      "source": [
        "\n",
        "This is one of multiple ECoG datasets from Miller 2019, recorded in clinical settings with a variety of tasks. Raw data here:\n",
        "\n",
        "https://exhibits.stanford.edu/data/catalog/zk881ps0522\n",
        "\n",
        "`dat` contain 4 sessions from 4 subjects, and was used in these papers:\n",
        "\n",
        "- Schalk, G., et al. \"Decoding two-dimensional movement trajectories using electrocorticographic signals in humans.\" Journal of Neural Engineering 4.3 (2007): 264. doi: [10.1088/1741-2560/4/3/012](https://doi.org/10.1088/1741-2560/4/3/012)\n",
        "\n",
        "- Schalk, Gerwin, et al. \"Two-dimensional movement control using electrocorticographic signals in humans.\" Journal of Neural Engineering 5.1 (2008): 75. doi: [10.1088/1741-2560/5/1/008](https://doi.org/10.1088/1741-2560/5/1/008)\n",
        "\n",
        "<br>\n",
        "\n",
        "From the dataset readme:\n",
        "\n",
        "*During the study, each patient was in a semi-recumbent position in a hospital bed about 1 m from a computer monitor. The patient used a joystick to maneuver a white cursor track a green target moving counter-clockwise in a circle of diameter 85% of monitor height ~1m away. The hand used to control the joystick was contralateral to the implanted electrode array.*\n",
        "\n",
        "<br>\n",
        "\n",
        "We also know that subject 0 was implanted in the left temporal lobe, while subject 2 was implanted in the right frontal lobe.\n",
        "\n",
        "Sample rate is always 1000Hz, and the ECoG data has been notch-filtered at 60, 120, 180, 240 and 250Hz, followed by z-scoring across the entire recording and conversion to float16 to minimize size.\n",
        "\n",
        "Variables are:\n",
        "* `dat['V']`: continuous voltage data (time by channels)\n",
        "* `dat['targetX']`: position of the target on the screen\n",
        "* `dat['targetY']`: position of the target on the screen\n",
        "* `dat['cursorX']`: X position of the cursor controlled by the joystick\n",
        "* `dat['cursorY']`: X position of the cursor controlled by the joystick\n",
        "* `dat['locs`]: three-dimensional coordinates of the electrodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74-rTHZXlfsb"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKHviLK3lfsb"
      },
      "outputs": [],
      "source": [
        "# DEFINING USEFUL CONSTANTS\n",
        "SAMPLE_RATE = 1000\n",
        "# By checking cursor sample rate in \"Identifying sample ...\" below, we figured\n",
        "# the sample rate of cursor on screen was identified every 40 time points\n",
        "TIMEPOINTS_PER_CURSOR_UPDATE = 40\n",
        "# Value for \"inconsistent\" bin when labeling by quadrants - see label_data_by_quadrant\n",
        "INCONSISTENCY_EXPRESSING_VALUE = 255 # max value of uint8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9idUHnrlfsb"
      },
      "outputs": [],
      "source": [
        "alldat = np.load(fname, allow_pickle=True)['dat']\n",
        "print(\"Shape of alldat:\", alldat.shape)\n",
        "\n",
        "# Select just one of the recordings here. This is subject 1, block 1.\n",
        "dat = alldat[0][0]\n",
        "print(dat.keys())\n",
        "print(\"Shape of V:\", dat['V'].shape)\n",
        "print(\"Shape of targetX:\", dat['targetX'].shape)\n",
        "print(\"Shape of targetY:\", dat['targetY'].shape)\n",
        "print(\"Shape of cursorX:\", dat['cursorX'].shape)\n",
        "print(\"Shape of cursorY:\", dat['cursorY'].shape)\n",
        "\n",
        "plt.plot(dat['V'][:, 0])\n",
        "plt.title(\"first voltage channel for subject 1, session 1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbbbg4_Xlfsc"
      },
      "outputs": [],
      "source": [
        "patient_number = 0\n",
        "\n",
        "dat = alldat[0][patient_number]\n",
        "V = dat['V'].astype('float16')\n",
        "\n",
        "nt, nchan = V.shape\n",
        "\n",
        "plt.plot(V)\n",
        "plt.xlabel('Time (10e-3s)')\n",
        "plt.ylabel(f'Norm V for P: {patient_number}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eQ-i9zalfsc"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/10\n",
        "\n",
        "def datatype_per_patient(patient_num):\n",
        "  print(f\"PATIENT {i}\")\n",
        "  patient = alldat[0][patient_num]\n",
        "  printed = \" \".join(\n",
        "      [f\"({k} : {patient[k].dtype})\" if isinstance(patient[k], np.ndarray) else \"\"\n",
        "      for k in patient.keys()])\n",
        "  print(printed)\n",
        "\n",
        "\n",
        "print(\"DATATYPE:\")\n",
        "for i in range(len(alldat[0])):\n",
        "  datatype_per_patient(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clGfNStKlfsc"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/10\n",
        "\n",
        "def description_for_patient_i(i):\n",
        "  print(f\"FOR PATIENT {i}:\")\n",
        "  patient = alldat[0][i]\n",
        "\n",
        "  assert(patient['V'].shape[0]       == patient['targetX'].shape[0] and\n",
        "         patient['targetX'].shape[0] == patient['targetY'].shape[0] and\n",
        "         patient['targetY'].shape[0] == patient['cursorX'].shape[0] and\n",
        "         patient['cursorX'].shape[0] == patient['cursorY'].shape[0])\n",
        "\n",
        "  assert(patient['targetX'].shape[1] == 1 and\n",
        "         patient['targetY'].shape[1] == 1 and\n",
        "         patient['cursorX'].shape[1] == 1 and\n",
        "         patient['cursorY'].shape[1] == 1)\n",
        "\n",
        "  assert(patient['V'].shape[1] == patient['locs'].shape[0])\n",
        "\n",
        "  num_timepoints, num_channels = patient['V'].shape\n",
        "  print(f\" - Voltage data has {num_timepoints} timepoints and {num_channels} channels.\")\n",
        "\n",
        "  print(f\" - Locations of {num_channels} electrodes expressed in {patient['locs'].shape[1]}D.\")\n",
        "\n",
        "  for category_label in ['hemisphere', 'lobe', 'gyrus', 'Brodmann_Area']:\n",
        "    unique_labels = np.unique(patient[category_label])\n",
        "    print(f\" - Unique labels for {category_label}: \\n{unique_labels}.\")\n",
        "\n",
        "  print(\"___________________________________\")\n",
        "\n",
        "\n",
        "num_patients = len(alldat[0])\n",
        "# print(f\"We have {num_patients} patients in this dataset.\")\n",
        "\n",
        "for i in range(num_patients):\n",
        "  description_for_patient_i(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH6-z2E2lfsd"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/10\n",
        "\n",
        "# Given the cursor movement sample rates seems lower than 1000Hz,\n",
        "# identify sample rate for cursor movement\n",
        "\n",
        "# first define useful functions\n",
        "# Counts the number of consecutive true / false blocks\n",
        "def count_consecutive_true_or_false_block_sizes(arr, countTrue):\n",
        "  # find positions where a new block begins\n",
        "  block_start = np.concatenate(([True], arr[:-1] != arr[1:]))\n",
        "  # get block lengths based on that\n",
        "  block_lengths = np.diff(\n",
        "      np.append( np.where(block_start)[0], len(arr))\n",
        "      )\n",
        "  # count block sizes of interest\n",
        "  count_start = 0 if (countTrue == arr[0]) else 1\n",
        "  counting_block_sizes = block_lengths[count_start::2]\n",
        "  return counting_block_sizes\n",
        "\n",
        "def SanityCheck_count_consecutive_true_or_false_block_sizes():\n",
        "  block_sizes = [3,2,5,1,20,3]; initial_val = False\n",
        "  initial_val_blocks = [3,5,20]; opposite_val_blocks = [2,1,3]\n",
        "  arr = np.concatenate(\n",
        "      [np.array([i % 2 != 0] * block_sizes[i])\n",
        "      for i in range(0, len(block_sizes))]\n",
        "  )\n",
        "  np.testing.assert_array_equal(opposite_val_blocks,\n",
        "      count_consecutive_true_or_false_block_sizes(arr, not initial_val))\n",
        "  np.testing.assert_array_equal(initial_val_blocks,\n",
        "      count_consecutive_true_or_false_block_sizes(arr, initial_val))\n",
        "\n",
        "SanityCheck_count_consecutive_true_or_false_block_sizes()\n",
        "\n",
        "# then use it\n",
        "def check_window_sizes(patient_num):\n",
        "  print(f\"PATIENT {patient_num}: \")\n",
        "  def check_for_window_size_given_one_array(arr):\n",
        "    # indices at which values are different from immediate previous\n",
        "    # * 1st entry is considered different\n",
        "    block_starts = np.insert(np.ediff1d(arr).astype(bool), 0, True)\n",
        "\n",
        "    likely_window_sizes = count_consecutive_true_or_false_block_sizes(block_starts, False) + 1\n",
        "    unique_window_sizes = np.unique(likely_window_sizes)\n",
        "    print(\"Unchanging window sizes found: \\n\", unique_window_sizes)\n",
        "    # at this point, we can see that values are all multiples of 40\n",
        "    assert np.prod(np.mod(unique_window_sizes, 40) == 0) == 1, \\\n",
        "           f\"Some window size for patient {patient_num} isn't a multiple of 40: \" + \\\n",
        "           f\"{arr}.\"\n",
        "\n",
        "  cursorX = alldat[0][patient_num][\"cursorX\"]\n",
        "  cursorY = alldat[0][patient_num][\"cursorY\"]\n",
        "  targetX = alldat[0][patient_num][\"targetX\"]\n",
        "  targetY = alldat[0][patient_num][\"targetY\"]\n",
        "\n",
        "  check_for_window_size_given_one_array(cursorX)\n",
        "  check_for_window_size_given_one_array(cursorY)\n",
        "  check_for_window_size_given_one_array(targetX)\n",
        "  check_for_window_size_given_one_array(targetY)\n",
        "\n",
        "for patient_num in range(len(alldat[0])):\n",
        "  check_window_sizes(patient_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD6Zcul1lfsd"
      },
      "source": [
        "## Dataset Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2fJYP2-lfsd"
      },
      "source": [
        "### Joystick Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrs-8jw1lfsd"
      },
      "outputs": [],
      "source": [
        "# plot cursor data\n",
        "dat = alldat[0][0]\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(dat['cursorX'], label='cursorX')\n",
        "plt.plot(dat['targetX'], label='targetX')\n",
        "plt.title('cursor and target over time (X direction)')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(dat['cursorY'], label='cursorY')\n",
        "plt.plot(dat['targetY'], label='targetY')\n",
        "plt.title('cursor and target over time (Y direction)')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvDv1vMPlfse"
      },
      "outputs": [],
      "source": [
        "plt.figure(3, figsize=(8, 8))\n",
        "plt.plot(dat['cursorX'][:10000], dat['cursorY'][:10000], label='cursor')\n",
        "plt.plot(dat['targetX'][:10000], dat['targetY'][:10000], label='target')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('first 10000 time steps')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nV0peV3lfse"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/10\n",
        "\n",
        "CALCULATION_DATATYPE = np.float32\n",
        "\n",
        "# Add labels based on X, Y directions of cursor at each point in time\n",
        "# At each time point, the cursor position is simulated as:\n",
        "# 1) direction as the direction from current position to next position\n",
        "# 2) distance from center as speed between time points\n",
        "# 2's definition is quite arbitrary - but we believe it is not an issue,\n",
        "# given we only consider the overall stick direction 1 as label.\n",
        "# Returns speed and direction (counter-clockwise starting from east direction\n",
        "# in radians) between each time point\n",
        "def get_speed_and_direction(cursorX : np.ndarray, cursorY : np.ndarray,\n",
        "                            xLimits=( float(\"-inf\"), float(\"inf\")),\n",
        "                            yLimits=( float(\"-inf\"), float(\"inf\"))):\n",
        "  if type(xLimits) not in [type([]), type(())] or len(xLimits) != 2:\n",
        "    print(\"xLimits must be a list/tuple with length 2; try again!\")\n",
        "    return\n",
        "  elif type(yLimits) not in [type([]), type(())] or len(yLimits) != 2:\n",
        "    print(\"yLimits must be a list/tuple with length 2; try again!\")\n",
        "    return\n",
        "  else:\n",
        "    xUpperbound = max(xLimits); xLowerbound = min(xLimits)\n",
        "    yUpperbound = max(yLimits); yLowerbound = min(yLimits)\n",
        "\n",
        "  cursorX = cursorX.astype(CALCULATION_DATATYPE)\n",
        "  cursorY = cursorY.astype(CALCULATION_DATATYPE)\n",
        "  # filter based on x/y limits\n",
        "  cursorX[(cursorX > xUpperbound) | (cursorX < xLowerbound)] = np.nan\n",
        "  cursorY[(cursorY > yUpperbound) | (cursorY < yLowerbound)] = np.nan\n",
        "  # compute speed & direction between each time point\n",
        "  diffX = np.ediff1d(cursorX); diffY = np.ediff1d(cursorY)\n",
        "  speedSq = np.square(diffX) + np.square(diffY)\n",
        "  speed = np.sqrt(speedSq)\n",
        "  # direction\n",
        "  direction = np.arctan2(diffY, diffX)\n",
        "  direction = np.mod(direction + 2*np.pi, 2*np.pi) # 0 <= dir <= 2pi\n",
        "  return speed, direction\n",
        "\n",
        "def SanityCheck_get_speed_and_direction():\n",
        "  # try 30, 45, 60, 120, 135, 150, 210, 225, 240, 300, 315 and 330 degrees\n",
        "  PI = np.pi; angles = [  PI/6,   PI/4, 2*PI/6,  4*PI/6, 3*PI/4,  5*PI/6,\n",
        "                        7*PI/6, 5*PI/4, 8*PI/6, 10*PI/6, 7*PI/4, 11*PI/6]\n",
        "  # try 1, 2 and 3 speeds\n",
        "  distances = [1,2,3]\n",
        "\n",
        "  for angle in angles:\n",
        "    for distance in distances:\n",
        "      x = np.array([0, distance*np.cos(angle)]).astype(CALCULATION_DATATYPE)\n",
        "      y = np.array([0, distance*np.sin(angle)]).astype(CALCULATION_DATATYPE)\n",
        "      speed, direction = get_speed_and_direction(x, y)\n",
        "      np.testing.assert_almost_equal(distance,  speed, decimal=5)\n",
        "      np.testing.assert_almost_equal(angle, direction, decimal=5)\n",
        "\n",
        "SanityCheck_get_speed_and_direction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rNH3zj-lfse"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/10\n",
        "\n",
        "# Plots joy stick position over time, with points progressively increasing in\n",
        "# shade darkness for later time points\n",
        "def plot_figure_over_time(X : np.ndarray, Y : np.ndarray,\n",
        "                          start : int, end : int, figureName : str):\n",
        "  blueColor = plt.cm.Blues(np.linspace(0.1,1,(end-start)))\n",
        "  fig,ax = plt.subplots(figsize=(6,6))\n",
        "  for k in range(end - start):\n",
        "    if k + 1 > len(X): break\n",
        "    ax.plot(X[k:k+2], Y[k:k+2], color=blueColor[k])\n",
        "  plt.xlabel('X'); plt.ylabel('Y')\n",
        "  plt.title(figureName)\n",
        "  plt.xlim(np.nanmin(X), np.nanmax(X)); plt.ylim(np.nanmin(Y), np.nanmax(Y))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwD-tbHJlfsf"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/10\n",
        "\n",
        "# Plots and saves a gif of the change of an object's X/Y position over time\n",
        "# X/Y : List of plot data over time\n",
        "# plot_every_n : Plot 1 every N values of the X, Y inputs.\n",
        "# figurepath : path to saving for gif, including name and extension\n",
        "# doSave : whether to save the gif\n",
        "# addProgressBar : whether to add a \"progress bar\" rendering\n",
        "def plot_and_save_gif(X, Y, plot_every_n : int, figurepath : str,\n",
        "                      doSave : bool=True, addProgressBar : bool = True,\n",
        "                      trail_length : int = None):\n",
        "\n",
        "  xLowerbound, xUpperbound = np.nanmin(X), np.nanmax(X)\n",
        "  yLowerbound, yUpperbound = np.nanmin(Y), np.nanmax(Y)\n",
        "  # Calculate how often we plot\n",
        "  number_of_rendered_frames = len(X) // plot_every_n\n",
        "  # Create the figure and axes objects\n",
        "  fig, ax = plt.subplots(figsize=(6,6))\n",
        "  # Set the initial plot\n",
        "  plt.title(f\"{os.path.basename(figurepath)}\")\n",
        "  plt.xlim(xLowerbound, xUpperbound)\n",
        "  plt.ylim(yLowerbound, yUpperbound)\n",
        "  data = ax.plot(X[0], Y[0], animated=True)[0]\n",
        "\n",
        "  # Set up progress bar\n",
        "  if addProgressBar:\n",
        "    progressBarXMin = -0.8; progressBarXMax = 0.8; progressBarY = -0.8\n",
        "\n",
        "    progressBarMiddle = (progressBarXMax + progressBarXMin) / 2\n",
        "    progressBar = ax.plot(progressBarXMin, progressBarY, animated=True)[0]\n",
        "    progressBarX = [(progressBarXMax - progressBarXMin) / number_of_rendered_frames * x -\n",
        "                    progressBarXMin\n",
        "                    for x in range(number_of_rendered_frames)]\n",
        "\n",
        "  def update(i):\n",
        "      plot_up_to = min(i * plot_every_n, len(X))\n",
        "\n",
        "      if trail_length is None:\n",
        "        plot_from = 0\n",
        "      else:\n",
        "        plot_from = max(plot_up_to - trail_length, 0)\n",
        "\n",
        "      data.set_data(X[plot_from:plot_up_to], Y[plot_from:plot_up_to])\n",
        "\n",
        "      if addProgressBar:\n",
        "        progressBar.set_data(progressBarX[:i], [progressBarY]*i)\n",
        "        return (data, progressBar)\n",
        "      else:\n",
        "        return (data,)\n",
        "\n",
        "  # Create the animation object\n",
        "  animation_fig = animation.FuncAnimation(\n",
        "      fig, update, frames=math.ceil(len(X) / plot_every_n), interval=30,\n",
        "      blit=True, repeat_delay=10\n",
        "      )\n",
        "\n",
        "  # Show the animation\n",
        "  plt.show()\n",
        "  if doSave:\n",
        "    animation_fig.save(figurepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNgxd-Helfsf"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/10\n",
        "\n",
        "# Plots and saves a gif of the change of the cursor and joystick X/Y position\n",
        "# over time\n",
        "# cursorX/cursorY : List of cursor plot data over time\n",
        "# joystickX/joystickY : List of joystick plot data over time\n",
        "# plot_every_n : Plot 1 every N values of the X, Y inputs.\n",
        "# figurepath : path to saving for gif, including name and extension\n",
        "# doSave : whether to save the gif\n",
        "# addProgressBar : whether to add a \"progress bar\" rendering\n",
        "def plot_and_save_cursor_and_joystick_gif(cursorX, cursorY, joystickX, joystickY,\n",
        "                                          plot_every_n : int, figurepath : str,\n",
        "                                          doSave : bool=True,\n",
        "                                          addProgressBar : bool = True,\n",
        "                                          trail_length : int = None):\n",
        "\n",
        "  xCursorLowerbound, xCursorUpperbound = np.nanmin(cursorX), np.nanmax(cursorX)\n",
        "  yCursorLowerbound, yCursorUpperbound = np.nanmin(cursorY), np.nanmax(cursorY)\n",
        "  xJoystickLowerbound, xJoystickUpperbound = np.nanmin(joystickX), np.nanmax(joystickX)\n",
        "  yJoystickLowerbound, yJoystickUpperbound = np.nanmin(joystickY), np.nanmax(joystickY)\n",
        "  # Calculate how often we plot\n",
        "  assert len(cursorX) == len(cursorY)\n",
        "  assert len(joystickX) == len(joystickY)\n",
        "  totalOriginalFrames = max(len(cursorX), len(joystickX))\n",
        "  number_of_rendered_frames = totalOriginalFrames // plot_every_n\n",
        "  # Create the figure and axes objects\n",
        "  fig, (c_ax, j_ax) = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
        "  plt.title(f\"{os.path.basename(figurepath)}\")\n",
        "  # Set the initial cursor plot\n",
        "  c_ax.set_xlim(xCursorLowerbound, xCursorUpperbound)\n",
        "  c_ax.set_ylim(yCursorLowerbound, yCursorUpperbound)\n",
        "  c_data = c_ax.plot(cursorX[0], cursorY[0], animated=True)[0]\n",
        "  # Set the initial joystick plot\n",
        "  j_ax.set_xlim(xJoystickLowerbound, xJoystickUpperbound)\n",
        "  j_ax.set_ylim(yJoystickLowerbound, yJoystickUpperbound)\n",
        "  j_data = j_ax.plot(joystickX[0], joystickY[0], animated=True)[0]\n",
        "\n",
        "  # Set up progress bar\n",
        "  if addProgressBar:\n",
        "    progressBarXMin = -0.8; progressBarXMax = 0.8; progressBarY = -0.8\n",
        "    progressBar = j_ax.plot(progressBarXMin, progressBarY, animated=True)[0]\n",
        "    progressBarX = [(progressBarXMax - progressBarXMin) /\n",
        "                    number_of_rendered_frames * x +\n",
        "                    progressBarXMin\n",
        "                    for x in range(number_of_rendered_frames)]\n",
        "\n",
        "  def update(i):\n",
        "      plot_up_to = min(i * plot_every_n, totalOriginalFrames)\n",
        "\n",
        "      if trail_length is None:\n",
        "        plot_from = 0\n",
        "      else:\n",
        "        plot_from = max(plot_up_to - trail_length, 0)\n",
        "\n",
        "      c_data.set_data(  cursorX[plot_from:plot_up_to],   cursorY[plot_from:plot_up_to])\n",
        "      j_data.set_data(joystickX[plot_from:plot_up_to], joystickY[plot_from:plot_up_to])\n",
        "      if addProgressBar:\n",
        "        progressBar.set_data(progressBarX[:i], [progressBarY]*i)\n",
        "        return (c_data, j_data, progressBar)\n",
        "      else:\n",
        "        return (c_data, j_data)\n",
        "\n",
        "  # Create the animation object\n",
        "  animation_fig = animation.FuncAnimation(\n",
        "      fig, update, frames=math.ceil(totalOriginalFrames / plot_every_n),\n",
        "      interval=100, blit=True, repeat_delay=10\n",
        "      )\n",
        "\n",
        "  # Show the animation\n",
        "  plt.show()\n",
        "  if doSave:\n",
        "    animation_fig.save(figurepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUe6Onj-Q1IV"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/31\n",
        "\n",
        "def process_cursor_data(patient_data,\n",
        "                        label_binsize : int=TIMEPOINTS_PER_CURSOR_UPDATE):\n",
        "    \"\"\"\n",
        "    Defines a standardized way of processing cursor data to obtain labels.\n",
        "    Cursor data is extracted for every 'label_binsize' timepoints, and any time\n",
        "    when the cursur position is at the ledge of the screen is replaced with nan.\n",
        "    Then, speed & direction is calculated from the remaining time series.\n",
        "    Finally, speeds above an arbitrary threshold (10000) are set to 0, as\n",
        "    impossible jitters.\n",
        "    :param dict patient_data: A dictionary holding patient data such as 'cursorX'.\n",
        "    :param int label_binsize: The stride at which we sample cursor position.\n",
        "    Must be a multiple of TIMEPOINTS_PER_CURSOR_UPDATE. Defaults to TIMEPOINTS_PER_CURSOR_UPDATE.\n",
        "    :returns speed, direction, joystickX, joystickY:\n",
        "    \"\"\"\n",
        "    cursorX = patient_data[\"cursorX\"][::label_binsize]\n",
        "    cursorY = patient_data[\"cursorY\"][::label_binsize]\n",
        "    # set ledges so that we exclude joystick position when cursor reaches\n",
        "    # ledges of screen\n",
        "    ledgesX = (np.nanmin(cursorX)+1, np.nanmax(cursorX)-1)\n",
        "    ledgesY = (np.nanmin(cursorY)+1, np.nanmax(cursorY)-1)\n",
        "\n",
        "    speed, direction = get_speed_and_direction(\n",
        "        cursorX, cursorY, xLimits=ledgesX, yLimits=ledgesY\n",
        "        )\n",
        "    # ARBITRARY REMOVAL OF VALUES OF SPEED OVER 10000; THIS MAKES THE GRAPH MUCH\n",
        "    # MORE EASY TO UNDERSTAND\n",
        "    speed[speed > 10000] = 0\n",
        "    norm_speed = speed / np.nanmax(speed) # normalize between 0 and 1\n",
        "\n",
        "    # plt.hist(speed)\n",
        "    # plt.yscale('log')\n",
        "    # plt.show\n",
        "\n",
        "    # compute position of joy stick from speed & direction\n",
        "    joystickX = norm_speed * np.cos(direction)\n",
        "    joystickY = norm_speed * np.sin(direction)\n",
        "\n",
        "    return speed, direction, joystickX, joystickY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxPzQCPglfsf"
      },
      "outputs": [],
      "source": [
        "# Plot joystick and cursor position side by side\n",
        "\n",
        "# SETUP\n",
        "start = 0\n",
        "end = start + 3000\n",
        "\n",
        "patient = alldat[0][0]\n",
        "cursorX = patient[\"cursorX\"][::TIMEPOINTS_PER_CURSOR_UPDATE]\n",
        "cursorY = patient[\"cursorY\"][::TIMEPOINTS_PER_CURSOR_UPDATE]\n",
        "\n",
        "speed, direction, joystickX, joystickY = process_cursor_data(\n",
        "    patient_data=patient,\n",
        "    label_binsize=TIMEPOINTS_PER_CURSOR_UPDATE)\n",
        "\n",
        "end = min(end, len(speed))\n",
        "\n",
        "# PLOT\n",
        "plot_figure_over_time(\n",
        "    joystickX, joystickY, start, end,\n",
        "    f'{start} to {min(end, len(joystickY))} time steps joystick position'\n",
        "    )\n",
        "\n",
        "# # rendering only joystick\n",
        "# FIGURE_PATH = \"joystick_movement.gif\"\n",
        "# plot_and_save_gif(joystickX[start:end], joystickY[start:end],\n",
        "#                   plot_every_n=10, figurepath=FIGURE_PATH, doSave=True,\n",
        "#                   addProgressBar=True, trail_length=10*10)\n",
        "# # rendering both cursor and joystick\n",
        "# FIGURE_PATH = \"cursor_and_joystick_movement.gif\"\n",
        "# plot_and_save_cursor_and_joystick_gif(\n",
        "#       cursorX[start:end],   cursorY[start:end],\n",
        "#     joystickX[start:end], joystickY[start:end],\n",
        "#     plot_every_n=10, figurepath=FIGURE_PATH, doSave=True,\n",
        "#     addProgressBar=True, trail_length=10*10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJviAK-Wlfsg"
      },
      "source": [
        "### ECoG Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfSLFiillfsg"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcWgOp_HryOP"
      },
      "source": [
        "## Dataset Labeling\n",
        "\n",
        "What we need to get decided:\n",
        "\n",
        "1.   What processing to further do on the data of format ```[patient x electrode x channel x time]```, aka general processing?\n",
        "\n",
        "2.   What features to extract (and how) from those features? e.g. Binning, feature extraction based on correlation, calculation of general features like local motor potential\n",
        "\n",
        "3. How to put those into a dataset that is loadable into pytorch? Pytorch customizable dataloaders can really load any format - maybe a numpy file is appropriate for our case. -> store post-processing datasets in the form of ```[patient x feature x bins]```, with labels, ready to be loaded?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED0g8wnFr8J8"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/10\n",
        "\n",
        "# TRIAL IMPLEMENTATION OF CUSTOM DATASET WILL COME HERE\n",
        "# USING [https://pytorch.org/tutorials/beginner/basics/data_tutorial.html] AS EXAMPLE\n",
        "\n",
        "def bin_data_to_binsize(data : Dict,\n",
        "                        binsize : int,\n",
        "                        stepsize : int,\n",
        "                        show_conversion : bool = False):\n",
        "  \"\"\"\n",
        "  Bins numpy arrays given as dictinary of data into given bin sizes\n",
        "  e.g. for a time series of [n x m] and bin size k and step size l, we produce a\n",
        "       [num_bins x k x m] array with entries ready for analysis\n",
        "       Here, num_bins = (n - k) // l + 1\n",
        "  \"\"\"\n",
        "  returned = {}\n",
        "\n",
        "  if binsize % TIMEPOINTS_PER_CURSOR_UPDATE != 0:\n",
        "    print(f\"You specified bin datasize {binsize} which isn't a multiple of \" +\n",
        "          f\"the frequency of cursor update that is {TIMEPOINTS_PER_CURSOR_UPDATE}. \")\n",
        "\n",
        "  if binsize < TIMEPOINTS_PER_CURSOR_UPDATE:\n",
        "    print(f\"You specified bin datasize {binsize} which is smaller than \" +\n",
        "          f\"the frequency of cursor update that is {TIMEPOINTS_PER_CURSOR_UPDATE}. \")\n",
        "    print(\"Labels associated to this binning might not have a meaningful interpretation.\")\n",
        "\n",
        "  for key, d in data.items():\n",
        "    binned_d = bin_array_using_binsize_and_stepsize(arr=d,\n",
        "                                                    binsize=binsize,\n",
        "                                                    stepsize=stepsize)\n",
        "    returned[f'{key}_binned'] = binned_d\n",
        "\n",
        "    if show_conversion:\n",
        "      print(f\"After reshaping: {binned_d.shape}\")\n",
        "      print(f\"Before reshaping: {d.shape}\")\n",
        "\n",
        "  return returned\n",
        "\n",
        "\n",
        "# Helper\n",
        "def bin_array_using_binsize_and_stepsize(arr : np.ndarray,\n",
        "                                         binsize : int,\n",
        "                                         stepsize : int):\n",
        "    arrsz = arr.shape[0]\n",
        "    # e.g. with array [1,2,3,4,5,6] and binsize=3, stepsize=2, bins are:\n",
        "    #      [1,2,3], [3,4,5] making truncated array size 5 = (6 - 3) // 2 * 2 + 3\n",
        "    arrsz = (arrsz - binsize) // stepsize * stepsize + binsize # rounding to nearest number of bins, truncating rest\n",
        "    arr = arr[:arrsz, ...] # truncate to nearest possible bin size\n",
        "\n",
        "    # if binsize is equal to stepsize, reshape (for fast performance?)\n",
        "    if binsize == stepsize:\n",
        "      newshape = (arrsz // binsize, binsize) + arr.shape[1:]\n",
        "      binned = arr.reshape(newshape)\n",
        "    else:\n",
        "      arr_list = []\n",
        "      for binstart in range(0, arrsz, stepsize):\n",
        "        # if entire bin is within arrsz range\n",
        "        binend = binstart + binsize - 1\n",
        "        if (binend + 1) <= arrsz:\n",
        "          arr_list.append(arr[binstart:binend+1])\n",
        "      binned = np.stack(arr_list, axis=0)\n",
        "\n",
        "    return binned\n",
        "\n",
        "def sanity_check_bin_data_to_binsize():\n",
        "  binsize = TIMEPOINTS_PER_CURSOR_UPDATE * 2\n",
        "  stepsize = TIMEPOINTS_PER_CURSOR_UPDATE // 2\n",
        "  # stepsize = TIMEPOINTS_PER_CURSOR_UPDATE * 2\n",
        "  binned_data = []\n",
        "\n",
        "  # for each patient index (0~3?)\n",
        "  for i, patient in enumerate(alldat[0]):\n",
        "    # likely useful labels: 'V', 'targetX/Y', 'cursorX/Y'\n",
        "    label_to_bin = ['V']\n",
        "    tobin = dict([ (key, patient[key]) for key in label_to_bin])\n",
        "    binned = bin_data_to_binsize(tobin, binsize, stepsize)\n",
        "    binned_data.append(binned)\n",
        "\n",
        "  print(f\"Chose bin size of {binsize} and step size of {stepsize}!\")\n",
        "  [print(f\"patient {i}:\", [f\"{key} - {patient[key].shape}\" for key in patient.keys()])\n",
        "  for i, patient in enumerate(binned_data)]\n",
        "\n",
        "  # also sanity check with a simple array\n",
        "  # [1,2,3,4,5,6], stepsize=2, binsize=3 -> [1,2,3], [3,4,5]\n",
        "  simple_array = np.array([[1,1],[2,2],[3,3],[4,4],[5,5],[6,6]])\n",
        "  binned = bin_data_to_binsize({'simple_arr' : simple_array}, binsize=3, stepsize=2)\n",
        "  # compare expected and actual\n",
        "  expected_binned = np.array([\n",
        "      [[1,1],[2,2],[3,3]],\n",
        "      [[3,3],[4,4],[5,5]]\n",
        "      ])\n",
        "  actual_binned = binned['simple_arr_binned']\n",
        "  assert np.array_equal(expected_binned, actual_binned), \"Result from bin_data_to_binsize was unexpected...\"\n",
        "  print(\"{}: {}\".format(*list(binned.items())[0]))\n",
        "\n",
        "sanity_check_bin_data_to_binsize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehw8KFycl0s9"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/04/03\n",
        "\n",
        "def label_data_by_quadrant(patient : Dict,\n",
        "                       binwidth : int=TIMEPOINTS_PER_CURSOR_UPDATE,\n",
        "                       stepwidth : int=TIMEPOINTS_PER_CURSOR_UPDATE):\n",
        "    \"\"\"\n",
        "    Given a specific 'patient' dictionary holding cursor data:\n",
        "    1 - preprocesses the cursor data using process_cursor_data with given\n",
        "        'patient' data and 'TIMEPOINTS_PER_CURSORUPDATE' as binwidth.\n",
        "    2 - label each timepoint data from preprocessing by the quadrant the joystick\n",
        "        stayed in within the duration.\n",
        "    3 - upon the labels, for every bins of binwidth 'binwidth' which are spaced\n",
        "        from timepoint 0 at a spacing of 'stepwidth':\n",
        "      3.1 - aggregate labels together into bins\n",
        "      3.2 - if the labels are coherent, return it as a single label - otherwise,\n",
        "            return a None label for the section.\n",
        "      E.g. Assume we had a cursor-position-time-series of length 35,\n",
        "           TIMEPOINTS_PER_CURSORUPDATE was 5 and binwidth: 15, stepwidth: 10.\n",
        "           This could yield 6 timepoints sampled at an interval of 5.\n",
        "           Assume the labels given to those timepoints are [1,1,1,1,2,2,2].\n",
        "           - Then, bin1: [1,1,1] is given label 1.\n",
        "           - bin2: [1,1,2] is given label None (given it isn't coherent).\n",
        "           - bin3: [2,2,2] is given label 2.\n",
        "           The returned result is [1,None,2].\n",
        "    * binwidth & stepwidth must be a multiple of TIMEPOINTS_PER_CURSOR_UPDATE.\n",
        "    :param Dict patient: A dictionary holding patient data (e.g. cursor position).\n",
        "    :param int binwidth: The amount of data we aggregate when labels are obtained\n",
        "    in order to examine for coherence and give a single label.\n",
        "    Must be a multiple of TIMEPOINTS_PER_CURSOR_UPDATE. Defaults to TIMEPOINTS_PER_CURSOR_UPDATE.\n",
        "    :param int stepwidth: The interval between the start of each bin of labels.\n",
        "    Must be a multiple of TIMEPOINTS_PER_CURSOR_UPDATE. Defaults to TIMEPOINTS_PER_CURSOR_UPDATE.\n",
        "    :returns np.ndarray labels: The labels for obtained bins, in shape [num_bins,].\n",
        "    :returns float consistent_percentage: The percentage of label bins found\n",
        "    to be consistent.\n",
        "    \"\"\"\n",
        "    # if binwidth / stepwidth aren't multiples of TIMEPOINTS_PER_CURSOR_UPDATE, error\n",
        "    if binwidth % TIMEPOINTS_PER_CURSOR_UPDATE != 0:\n",
        "        raise Exception(f\"binwidth {binwidth} should be a multiple of \" +\n",
        "                        f\"{TIMEPOINTS_PER_CURSOR_UPDATE} in order for \"\n",
        "                         \"label_data_by_quadrant to run correctly...\")\n",
        "    if stepwidth % TIMEPOINTS_PER_CURSOR_UPDATE != 0:\n",
        "        raise Exception(f\"stepwidth {stepwidth} should be a multiple of \" +\n",
        "                        f\"{TIMEPOINTS_PER_CURSOR_UPDATE} in order for \"\n",
        "                         \"label_data_by_quadrant to run correctly...\")\n",
        "\n",
        "    # first preprocess data as specified by process_cursor_data\n",
        "    speed, direction, joystickX, joystickY = process_cursor_data(\n",
        "        patient_data=patient,\n",
        "        label_binsize=TIMEPOINTS_PER_CURSOR_UPDATE)\n",
        "    # then get the labels\n",
        "    labels = label_quadrant(speed, direction)\n",
        "    # next, bin labels and assign label based on consistency\n",
        "    binned_labels = bin_array_using_binsize_and_stepsize(\n",
        "        labels,\n",
        "        binsize = binwidth // TIMEPOINTS_PER_CURSOR_UPDATE,\n",
        "        stepsize=stepwidth // TIMEPOINTS_PER_CURSOR_UPDATE)\n",
        "\n",
        "    def check_for_consistency_and_return_label(bin : np.ndarray):\n",
        "        \"\"\"Returns a label if it's consistent within bin, None otherwise.\"\"\"\n",
        "        if np.all(bin[0] == bin[:]): #if bin is consistent\n",
        "            return bin[0]\n",
        "        else:\n",
        "            return INCONSISTENCY_EXPRESSING_VALUE\n",
        "\n",
        "    consistent_labels, consistent_count = [], 0\n",
        "    for bin_idx in range(binned_labels.shape[0]):\n",
        "        bin_idx_th_label = check_for_consistency_and_return_label(\n",
        "            binned_labels[bin_idx, :])\n",
        "        consistent_labels.append(bin_idx_th_label)\n",
        "        if bin_idx_th_label != INCONSISTENCY_EXPRESSING_VALUE:\n",
        "            consistent_count += 1\n",
        "\n",
        "    consistent_percentage = consistent_count / binned_labels.shape[0] * 100\n",
        "\n",
        "    return np.array(consistent_labels), consistent_percentage\n",
        "\n",
        "def label_quadrant(speed : np.ndarray, direction : np.ndarray):\n",
        "  \"\"\"\n",
        "  We divide the field into four quadrants; each quadrant is centered around\n",
        "  the four directions, 1:right, 2:up, 3:left, 4:down.\n",
        "  \"\"\"\n",
        "  # TODO - DO SOMETHING WITH SPEED!!\n",
        "  def angle_to_quadrant(direction):\n",
        "    if 0 <= direction <= np.pi/4:\n",
        "      quadrant = 1\n",
        "    elif np.pi/4 < direction <= np.pi*3/4:\n",
        "      quadrant = 2\n",
        "    elif np.pi*3/4 < direction <= np.pi*5/4:\n",
        "      quadrant = 3\n",
        "    elif np.pi*5/4 < direction <= np.pi*7/4:\n",
        "      quadrant = 4\n",
        "    else:\n",
        "      quadrant = 1\n",
        "    return quadrant\n",
        "\n",
        "  labels = np.vectorize(angle_to_quadrant)(direction).astype(np.uint8)\n",
        "  return labels\n",
        "\n",
        "def sanity_check_plot_labels_by_quadrant(X : np.ndarray,\n",
        "                                         Y : np.ndarray,\n",
        "                                         label : np.ndarray,\n",
        "                                         figureName : str):\n",
        "  fig = plt.figure(figsize=(6,6))\n",
        "  colors = ['black', 'purple', 'green', 'red', 'blue']\n",
        "\n",
        "  last_lbl, to_plot_start = label[0], 0\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    lbl = label[i] if label[i] != INCONSISTENCY_EXPRESSING_VALUE else 0\n",
        "\n",
        "    if last_lbl != lbl or (i+2 == len(X) or i+2 == len(label)): # start of new sequence, or end\n",
        "        # first plot what was already there\n",
        "        clr = colors[last_lbl]\n",
        "        plt.plot(X[to_plot_start:i+1], Y[to_plot_start:i+1], color=clr)\n",
        "        # start a new sequence\n",
        "        last_lbl, to_plot_start = lbl, i\n",
        "\n",
        "        if i+2 == len(X) or i+2 == len(label): break\n",
        "\n",
        "  plt.xlabel('X'); plt.ylabel('Y')\n",
        "  plt.title(figureName)\n",
        "  plt.xlim(np.nanmin(X), np.nanmax(X)); plt.ylim(np.nanmin(Y), np.nanmax(Y))\n",
        "  plt.show()\n",
        "\n",
        "# sanity check for label_quadrant\n",
        "label = label_quadrant(speed, direction)\n",
        "sanity_check_plot_labels_by_quadrant(joystickX, joystickY, label, figureName=\"Plot by quadrants\")\n",
        "\n",
        "# sanity check for label_data_by_quadrant\n",
        "MULTIPLIER=4\n",
        "label2, percentage = label_data_by_quadrant(patient=alldat[0][0],\n",
        "                                        binwidth =TIMEPOINTS_PER_CURSOR_UPDATE*MULTIPLIER,\n",
        "                                        stepwidth=TIMEPOINTS_PER_CURSOR_UPDATE)\n",
        "enhanced_label2 = np.vstack([arr for arr in [label2]*MULTIPLIER]).T.flatten()\n",
        "\n",
        "print(f\"{percentage}% of the bins were found to be consistent for patient 0!\")\n",
        "sanity_check_plot_labels_by_quadrant(joystickX, joystickY, label2,\n",
        "                                     figureName=\"Plot by quadrants - from patient data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q3kauBky94_"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/10\n",
        "def get_label_consistency_of_binsize(cursorX : np.ndarray,\n",
        "                                     cursorY : np.ndarray,\n",
        "                                     binsize  : int=TIMEPOINTS_PER_CURSOR_UPDATE,\n",
        "                                     max_multiple : int=10):\n",
        "  \"\"\"\n",
        "  For bin sizes that are up to 'max_multiple' multiple, calculate what fraction\n",
        "  of the bin is consistent in label.\n",
        "  :param np.ndarray cursorX: X cursor position extracted from the dataset.\n",
        "  :param np.ndarray cursorY: Y cursor position extracted from the dataset.\n",
        "  :param int binsize: Size of bin that is the minimum bin size we investigate.\n",
        "  :param int max_multiple: Investigate (i * 'binsize') bin sizes, where i is\n",
        "  between 2 and max_multiple inclusive.\n",
        "  \"\"\"\n",
        "  # first extract labels using binsize\n",
        "  speed, direction, joystickX, joystickY = process_cursor_data(\n",
        "    patient_data= { \"cursorX\" : cursorX, \"cursorY\" : cursorY},\n",
        "    label_binsize=binsize)\n",
        "\n",
        "  labels = label_quadrant(speed, direction)\n",
        "  consistent_proportions = []\n",
        "  # then check for multiples of binsize whether the labels match within bins\n",
        "  for multiplier in range(2, max_multiple+1):\n",
        "    consistent_bin_count = 0\n",
        "    # bin the label\n",
        "    labels_per_position_in_bin = bin_array_using_binsize_and_stepsize(\n",
        "        arr=labels, binsize=multiplier, stepsize=multiplier)\n",
        "\n",
        "    # count the number of consistent bins\n",
        "    for i in range(labels_per_position_in_bin.shape[0]):\n",
        "      # if bin is consistent\n",
        "      if np.all(labels_per_position_in_bin[i,0] == labels_per_position_in_bin[i,0:]):\n",
        "        consistent_bin_count += 1\n",
        "    # compute the ratio of consistent bins to the total\n",
        "    consistent_proportions.append(consistent_bin_count / labels_per_position_in_bin.shape[0])\n",
        "\n",
        "  return consistent_proportions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcGG-5XUCY7D"
      },
      "outputs": [],
      "source": [
        "patient = alldat[0][0]\n",
        "cursorX, cursorY = patient[\"cursorX\"], patient[\"cursorY\"]\n",
        "consistent_ratio = get_label_consistency_of_binsize(cursorX, cursorY, binsize=40)\n",
        "print(consistent_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fx_vebjbO78H"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: -\n",
        "# Last updated: 2024/03/10\n",
        "def save_npz_bins_and_labels_for_all_patients(original_dataset,\n",
        "                                              savedir : str,\n",
        "                                              filename : str=\"binned_V_and_label\",\n",
        "                                              V_binsize     : int=TIMEPOINTS_PER_CURSOR_UPDATE,\n",
        "                                              V_stepsize    : int=TIMEPOINTS_PER_CURSOR_UPDATE,\n",
        "                                              label_binsize : int=TIMEPOINTS_PER_CURSOR_UPDATE):\n",
        "  \"\"\"\n",
        "  Create a npz holding both binned data and their labels for all patients.\n",
        "  Voltage data will be binned according to both bin and step size.\n",
        "  A corresponding label array is provided, where a label is applied to the nth\n",
        "  voltage bin only if it is consistent within the duration, and otherwise a -1\n",
        "  label is applied.\n",
        "  :param dict original_dataset: The original dataset, as extracted from\n",
        "  joystick_track.npz.\n",
        "  :param str savedir: Directory to save the results to.\n",
        "  :param str filename: Name of the file we save the result as, which will be\n",
        "  suffixed with '_Vb{V_binsize}_Vs{V_stepsize}_l{label_binsize}'.\n",
        "  :param int V_binsize: The size to which we bin voltage data into. Must be a\n",
        "  multiple of label_binsize. Defaults to TIMEPOINTS_PER_CURSOR_UPDATE, and must\n",
        "  not be smaller.\n",
        "  :param int V_stepsize: The step size between two consecutive voltage data bins.\n",
        "  Must be a multiple of label_binsize. Defaults to TIMEPOINTS_PER_CURSOR_UPDATE,\n",
        "  and must not be smaller.\n",
        "  :param int label_binsize: The number of timesteps between two consecutive\n",
        "  cursor timepoints we use to identify the corresponding label. Recommended to\n",
        "  leave as default, TIMEPOINTS_PER_CURSOR_UPDATE, and must not be smaller.\n",
        "  \"\"\"\n",
        "  if V_binsize % label_binsize != 0:\n",
        "    raise Exception(f\"V_binsize {V_binsize} must be a multiple of label_binsize {label_binsize}!\")\n",
        "  if V_stepsize % label_binsize != 0:\n",
        "    raise Exception(f\"V_stepsize {V_stepsize} must be a multiple of label_binsize {label_binsize}!\")\n",
        "\n",
        "  toNpz_dict = {}\n",
        "\n",
        "  for i, patient in enumerate(original_dataset[0]):\n",
        "    # extract labels first\n",
        "    speed, direction, joystickX, joystickY = process_cursor_data(\n",
        "        patient_data=patient,\n",
        "        label_binsize=label_binsize)\n",
        "    label = label_quadrant(speed, direction)\n",
        "\n",
        "    # then bin it according to binsize and stepsize\n",
        "    labels_per_Vbin, labels_per_Vstep = V_binsize // label_binsize, V_stepsize // label_binsize\n",
        "    binned_label = bin_array_using_binsize_and_stepsize(arr=label,\n",
        "                                                        binsize=labels_per_Vbin,\n",
        "                                                        stepsize=labels_per_Vstep)\n",
        "    # labels consistent within a bin are preserved, and non-consistent bins are\n",
        "    # labeled as -1\n",
        "    consistent_label = []\n",
        "    for i in range(binned_label.shape[0]):\n",
        "      # if bin is consistent, keep the label\n",
        "      if np.all(binned_label[i,0] == binned_label[i,0:]):\n",
        "        consistent_label.append(binned_label[i,0])\n",
        "      # if bin isn't consistent, label it with -1\n",
        "      else:\n",
        "        consistent_label.append(-1)\n",
        "    consistent_label = np.array(consistent_label)\n",
        "\n",
        "    toNpz_dict[f\"patient_{i}_label\"] = consistent_label\n",
        "    # also bin dataset into the according format\n",
        "    label_to_bin = ['V']\n",
        "    tobin = dict([ (key, patient[key]) for key in label_to_bin])\n",
        "    V_binned = bin_data_to_binsize(tobin, V_binsize, V_stepsize, show_conversion=True)\n",
        "\n",
        "    toNpz_dict[f\"patient_{i}_V\"] = V_binned['V_binned']\n",
        "\n",
        "\n",
        "  final_filename = f\"{filename}_Vb{V_binsize}_Vs{V_stepsize}_l{label_binsize}\"\n",
        "  print(f\"Saving {final_filename} under {savedir}.\")\n",
        "  np.savez(os.path.join(savedir, final_filename), **toNpz_dict)\n",
        "  print(\"Successful.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKylz4P9UUDc"
      },
      "outputs": [],
      "source": [
        "# Actually save an instance of binned data\n",
        "\n",
        "# save computed result\n",
        "SAVE_DIR = \".\"\n",
        "save_npz_bins_and_labels_for_all_patients(\n",
        "    alldat,\n",
        "    savedir=SAVE_DIR,\n",
        "    filename=\"binned_V_and_label\",\n",
        "    V_binsize=40,\n",
        "    V_stepsize=40,\n",
        "    label_binsize=40\n",
        "    )\n",
        "\n",
        "# load and check result\n",
        "npzfile = np.load(\"binned_V_and_label_Vb40_Vs40_l40.npz\")\n",
        "print(f\"npzfile.files: {npzfile.files}\")\n",
        "for f in npzfile.files:\n",
        "  print(f\"npzfile[{f}].shape: {npzfile[f].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee8VqiPTjFxC"
      },
      "outputs": [],
      "source": [
        "patient = alldat[0][0]\n",
        "\n",
        "speed, direction, joystickX, joystickY = process_cursor_data(\n",
        "    patient_data=patient, label_binsize=TIMEPOINTS_PER_CURSOR_UPDATE)\n",
        "\n",
        "labels = label_quadrant(speed, direction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FOYFgBptVK3"
      },
      "source": [
        "## Feature Extraction\n",
        "----------\n",
        "Copying this paper [here](https://iopscience.iop.org/article/10.1088/1741-2552/ac4ed1/pdf):\n",
        "\n",
        "**Lin Yao et al (2022). Fast and accurate decoding of finger movements\n",
        "from ECoG through Riemannian features and\n",
        "modern machine learning techniques**\n",
        "\n",
        "<br>\n",
        "\n",
        "Signal from each electrode are:\n",
        "*   Common average referenced\n",
        "*   Divided into 200 ms epochs with 40 ms steps <- *ROOM FOR TWEAKING*\n",
        "\n",
        "Extracted features:\n",
        "*   alpha (8–13 Hz)\n",
        "*   beta (13–30 Hz)\n",
        "*   low-gamma (30–60 Hz)\n",
        "*   gamma (60–100 Hz)\n",
        "*   high-gamma (100–200 Hz) power\n",
        "*   LMPs - running average of raw ECoG for each channel\n",
        "*   Hjorth activity, mobility & complexity parameters - statistical properties: variance (activity), mean frequency (mobility) & changes in frequency over a given time period (complexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ycm30O3t2TC"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: 2024/03/13\n",
        "# Last updated: 2024/03/13\n",
        "\n",
        "def common_average_reference(V_data : np.ndarray, channel_axis : int=-1):\n",
        "    \"\"\"\n",
        "    Computes the common average referenced voltage data, which allows removal\n",
        "    of non-local brain activity to be removed and enhances signals.\n",
        "    For each time point t, the common-average-referenced voltage data for\n",
        "    channel n, t_rn is computed as:\n",
        "    t_rn = t_n - 1/N * (Sum from i=1~N) t_N\n",
        "    where t_n is the raw voltage data from channel n and we have N channels.\n",
        "    :param np.ndarray V_data: Voltage data which we common-average-reference.\n",
        "    :param int channel_axis: The axis that holds the channel dimension.\n",
        "    Defaults to -1.\n",
        "    :returns np.ndarray cmn_avg_ref: Common-average-referenced signal of same\n",
        "    shape as the input.\n",
        "    \"\"\"\n",
        "    common_average = np.mean(V_data, axis=channel_axis, keepdims=True)\n",
        "    cmn_avg_rfr = V_data - common_average\n",
        "    return cmn_avg_rfr\n",
        "\n",
        "def sanity_check_common_averge_reference():\n",
        "   pre_reference  = np.array([[ 0,1,2], [ 1,3,5], [ 4,7,10], [1,1,1]])\n",
        "   post_reference = np.array([[-1,0,1], [-2,0,2], [-3,0, 3], [0,0,0]])\n",
        "   cmn_avg_rfr = common_average_reference(pre_reference)\n",
        "   assert np.array_equal(cmn_avg_rfr, post_reference)\n",
        "   print(f\"Original array: {pre_reference}\")\n",
        "   print(f\"Common-average-referenced: {cmn_avg_rfr}\")\n",
        "\n",
        "sanity_check_common_averge_reference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh_4SldxPn5K"
      },
      "outputs": [],
      "source": [
        "patient = alldat[0][0]\n",
        "\n",
        "plt.plot(patient['V'][:, 0])\n",
        "plt.title(\"First voltage channel for subject 1, session 1\")\n",
        "plt.show()\n",
        "\n",
        "V_cm_avg_ref = common_average_reference(V_data=patient['V'])\n",
        "\n",
        "plt.plot(V_cm_avg_ref[:, 0])\n",
        "plt.title(\"Common-average-referenced first voltage channel for subject 1, session 1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anhuptvWvHFv"
      },
      "source": [
        "### Frequency Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4fnI9sb0EpY"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: 2024/03/10\n",
        "# Last updated: 2024/03/23\n",
        "\n",
        "# Conversion into frequency domain: can be done 1) by bins, or 2) for whole signal?\n",
        "# My vague understanding: smaller window = sharper signals, but less bands. Using\n",
        "#  larger windows will in turn yield many bands, but fails to capture momentous\n",
        "#  frequencies within the data. So we might wanna focus on smaller bins?\n",
        "#  Source on windowsize: [https://digitalsoundandmusic.com/2-3-10-windowing-the-fft/#:~:text=There's%20a%20tradeoff%20in%20the,size%20N%20is%20N%2F2.]\n",
        "def extract_frequency_features_from_binned_voltage(\n",
        "    binned_V : np.ndarray,\n",
        "    frequency_ranges : Tuple[Tuple[int]]\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Extract frequency features specified under 'frequency_ranges', given an\n",
        "    already binned voltage data. Fast fourier transform is used to convert\n",
        "    individual bins into their frequency domain.\n",
        "    :param np.ndarray binned_V: A binned voltage data array, of shape\n",
        "    [binnumber x binsize x channelsize].\n",
        "    :param Tuple[Tuple[int]] frequency_ranges: A tuple of tuple of integer\n",
        "    expressing the frequency ranges as ((low, high), (low, high), ...) etc.\n",
        "    Corresponding frequency ranges will be extracted, where the size of frequency\n",
        "    bins is determined by the bin size N of voltage data (N/2, if I understand correctly?)\n",
        "\n",
        "    I checked out https://docs.scipy.org/doc/scipy-1.11.4/tutorial/fft.html#d-discrete-fourier-transforms\n",
        "    for understanding how to apply fourier transforms from scipy.\n",
        "    \"\"\"\n",
        "    frequency_result, exact_freq_ranges = [], []\n",
        "\n",
        "    binsize = binned_V.shape[1]\n",
        "    # use rfft as we have real-valued inputs; speed up!\n",
        "    frequency_domain = fft.rfft(binned_V, axis=1)[:,1:,:] # exclude first term which isn't useful??\n",
        "    frequency_bincenters = fft.fftfreq(n=binsize, d=1/SAMPLE_RATE)\n",
        "    # truncate frequency_bincenters given it also returns negative frequency bins\n",
        "    pos_freq_bincenters = frequency_bincenters[frequency_bincenters > 0]\n",
        "    bin_width = (pos_freq_bincenters[1] - pos_freq_bincenters[0]).item()\n",
        "    bin_start = pos_freq_bincenters[0] - bin_width / 2\n",
        "    # extract requested frequency ranges as the largest set of bins that completely\n",
        "    # fits the requested range. If we cannot fit even a single bin, return the bin\n",
        "    # with most appropriate bin center\n",
        "    for rnglow, rnghigh in frequency_ranges:\n",
        "      idx_bincenter_in_rng_low  = int((rnglow  - bin_start) // bin_width)\n",
        "      idx_bincenter_in_rng_high = int((rnghigh - bin_start) // bin_width)\n",
        "      # if lowest/highest bin that has its center within range doesn't entirely fit\n",
        "      lowest_bin_not_fitting  = (pos_freq_bincenters[idx_bincenter_in_rng_low]  - bin_width / 2) < rnglow\n",
        "      highest_bin_not_fitting = (pos_freq_bincenters[idx_bincenter_in_rng_high] + bin_width / 2) > rnghigh\n",
        "      # deterine which bins are entirely fitting within range\n",
        "      idx_bin_in_rng_low  = idx_bincenter_in_rng_low\n",
        "      idx_bin_in_rng_high = idx_bincenter_in_rng_high\n",
        "      if  lowest_bin_not_fitting:  idx_bin_in_rng_low += 1\n",
        "      if highest_bin_not_fitting: idx_bin_in_rng_high -= 1\n",
        "\n",
        "      # if there is less then one bin fitting\n",
        "      if idx_bin_in_rng_low > idx_bin_in_rng_high:\n",
        "        # find and return the bin with closest bincenter to this range\n",
        "        rngcenter = (rnghigh + rnglow) / 2\n",
        "        if abs(pos_freq_bincenters[idx_bin_in_rng_low]  - rngcenter) < \\\n",
        "           abs(pos_freq_bincenters[idx_bin_in_rng_high] - rngcenter):\n",
        "            idx_bin_in_rng_high = idx_bin_in_rng_low\n",
        "        else:\n",
        "            idx_bin_in_rng_low = idx_bin_in_rng_high\n",
        "\n",
        "      # get the result\n",
        "      frequency_result.append(\n",
        "          2.0 / binsize * np.sum(\n",
        "              np.abs(\n",
        "                  frequency_domain[:,idx_bin_in_rng_low : idx_bin_in_rng_high + 1,:]\n",
        "                  ), axis=1\n",
        "              )\n",
        "          )\n",
        "\n",
        "      exact_freq_ranges.append(\n",
        "          (pos_freq_bincenters[idx_bin_in_rng_low].item()  - bin_width / 2,\n",
        "           pos_freq_bincenters[idx_bin_in_rng_high].item() + bin_width / 2)\n",
        "          )\n",
        "\n",
        "    return frequency_result, exact_freq_ranges\n",
        "\n",
        "def sanity_check_extract_frequency_features_from_binned_voltage():\n",
        "    def create_sine_wave_of_given_frequency_and_amplitude(freq  : float,\n",
        "                                                          amp   : float,\n",
        "                                                          oneHz_start : float,\n",
        "                                                          oneHz_stop  : float):\n",
        "      \"\"\"\n",
        "      Produces a wave that corresponds to a 'freq'-Hz sine wave with max\n",
        "      amplitude 'amp', which would overlap with a 1Hz wave that goes from\n",
        "      'oneHz_start' to 'oneHz_stop'.\n",
        "      e.g. a 2Hz wave with 'oneHz_start'=0 ~ 'oneHz_stop'=2pi goes from\n",
        "      0~4pi.\n",
        "      :param float freq: Frequency in Hz of wave.\n",
        "      :param float amp: Amplitude of sine wave to create.\n",
        "      :param float oneHz_start: Start position of a corresponding 1Hz sine wave\n",
        "      in radians. e.g. a 2Hz wave would go from '2*oneHz_start'.\n",
        "      :param float oneHz_stop: Stop position of a corresponding 1Hz sine wave\n",
        "      in radians. e.g. a 2Hz wave would go all the way to '2*oneHz_stop'.\n",
        "      :returns np.ndarray wave: The wave function of interest, [num_samples,].\n",
        "      :returns np.ndarray wave_rng: The timepoints corresponding to the wave\n",
        "      range, [num_samples,].\n",
        "      \"\"\"\n",
        "      num_samples = int(1000 * abs(oneHz_stop - oneHz_start) / (2*np.pi)) # 1000 samples / full cycle\n",
        "      freq_adjusted_start, freq_adjusted_stop = oneHz_start * freq, oneHz_stop * freq\n",
        "      wave_rng = np.linspace(start=freq_adjusted_start, stop=freq_adjusted_stop, num=num_samples)\n",
        "      print(\"The wave ranges {}-{}, sampled {} times.\".format(\n",
        "          round(freq_adjusted_start, 1), round(freq_adjusted_stop, 1), num_samples\n",
        "      ))\n",
        "      wave = np.sin(wave_rng) * amp\n",
        "      return wave, wave_rng\n",
        "\n",
        "    # create an artificial sine wave over periods 0~3 seconds as addition of:\n",
        "    # - a 10 Hz sine wave of amplitude 5\n",
        "    # - a 20 Hz sine wave of amplitude 4\n",
        "    # - a 45 Hz sine wave of amplitude 3\n",
        "    # - a 80 Hz sine wave of amplitude 2\n",
        "    # - a 150 Hz sine wave of amplitude 1\n",
        "    START_SEC, STOP_SEC = 0, 3\n",
        "    frequencies = [10, 20, 45, 80, 150]\n",
        "    amplitudes = [5, 4, 3, 2, 1]\n",
        "    artificial_sine = None\n",
        "    for freq, amp in zip(frequencies, amplitudes):\n",
        "        new_wave, wave_rng = create_sine_wave_of_given_frequency_and_amplitude(\n",
        "            freq=freq,\n",
        "            amp=amp,\n",
        "            oneHz_start=START_SEC * 2 * np.pi,\n",
        "            oneHz_stop = STOP_SEC * 2 * np.pi)\n",
        "        if artificial_sine is None:\n",
        "            artificial_sine = new_wave\n",
        "        else:\n",
        "            artificial_sine += new_wave\n",
        "        # show the artificial wave\n",
        "        print(f\"Just added wave of frequency {freq} and amplitude {amp}!\")\n",
        "        plt.plot(wave_rng, artificial_sine)\n",
        "        plt.title(f\"Artificial sine wave added up to frequency {freq}\")\n",
        "        plt.show()\n",
        "\n",
        "    # then, given the artificial wave, extract frequencies from each bin\n",
        "    tobin = {'V' : np.expand_dims(artificial_sine, axis=1)} #add channel dim (1)\n",
        "    # binsize & stepsize as generally expected for our usecase\n",
        "    V_binned = bin_data_to_binsize(tobin, binsize=200, stepsize=40, show_conversion=True)\n",
        "    V_binned = V_binned['V_binned']\n",
        "    # same for frequency ranges\n",
        "    RANGES =     ( (  8,  13), (  13,  30), (  30,  60), (  60, 100), (  100,  200) )\n",
        "    EXP_RANGES = ( (7.5,12.5), (17.5,27.5), (32.5,57.5), (62.5,97.5), (102.5,197.5) )\n",
        "    frequency_result, exact_freq_ranges = extract_frequency_features_from_binned_voltage(\n",
        "        binned_V=V_binned, frequency_ranges=RANGES\n",
        "        )\n",
        "\n",
        "    # check that the extracted frequency results & ranges match what's expected\n",
        "    for exp_rng, act_rng in zip(EXP_RANGES, exact_freq_ranges):\n",
        "      assert exp_rng == act_rng, \"Expected and actual range are different...\"\n",
        "\n",
        "    _, ax = plt.subplots()\n",
        "    bin_centers = [(max + min) / 2 for max, min in exact_freq_ranges]\n",
        "    np_freq_result = np.array(frequency_result)\n",
        "\n",
        "    print(f\"The expected 'bin-centers : amplitudes' are: \" + \\\n",
        "          f\"{['{}:{}'.format(freq, amp) for freq, amp in zip(frequencies, amplitudes)]}\")\n",
        "    num_timepoints = frequency_result[0].shape[0]\n",
        "    for i in range(num_timepoints):\n",
        "        freq_res_i = np_freq_result[:, i, 0] # all ranges, ith timepoint, 0th channel\n",
        "        ax.plot(bin_centers, freq_res_i)\n",
        "    plt.title(f\"Amplitude distributions for timepoints over bin centers\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    for exp_amp, act_amp in zip(amplitudes, frequency_result):\n",
        "      assert np.allclose(exp_amp, act_amp, atol=1e-1), \"Expected and actual amplitude per range are different ...\"\n",
        "\n",
        "\n",
        "sanity_check_extract_frequency_features_from_binned_voltage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgt3E5Q8JNnf"
      },
      "outputs": [],
      "source": [
        "patient = alldat[0][0]\n",
        "label_to_bin = ['V']\n",
        "tobin = dict([ (key, patient[key]) for key in label_to_bin])\n",
        "V_binned = bin_data_to_binsize(tobin, binsize=200, stepsize=40, show_conversion=True)\n",
        "\n",
        "V_binned = V_binned['V_binned']\n",
        "V_cm_avg_ref = common_average_reference(V_data=V_binned)\n",
        "\n",
        "RANGES = ( (8,13),\n",
        "           (13,30),\n",
        "           (30,60),\n",
        "           (60,100),\n",
        "           (100,200) )\n",
        "\n",
        "frequency_result, exact_freq_ranges = extract_frequency_features_from_binned_voltage(\n",
        "    binned_V=V_cm_avg_ref, frequency_ranges=RANGES\n",
        "    )\n",
        "print(\"Activity bins: \")\n",
        "for res, rng in zip(frequency_result, exact_freq_ranges):\n",
        " print(\"{}:{}\".format(res.shape, rng))\n",
        "\n",
        "FREQUENCY_N, CHANNEL_N = 0, 0\n",
        "plt.plot(frequency_result[FREQUENCY_N][:,CHANNEL_N])\n",
        "plt.title(f\"Example, frequency bin {'{} to {}'.format(*exact_freq_ranges[FREQUENCY_N])} \" +\n",
        "          f\"over time for channel {CHANNEL_N}\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plBxUfessRUM"
      },
      "source": [
        "### Local Motor Potential (LMP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd3RflmLsVU1"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: 2024/03/13\n",
        "# Last updated: 2024/03/14\n",
        "\n",
        "def extract_lmp_from_binned_voltage(binned_V : np.ndarray):\n",
        "    \"\"\"\n",
        "    Extracts the local motor potential feature from binned\n",
        "    voltage data. Local motor potential refers to voltage data\n",
        "    that is amplitude-tuned to cursor movements in the time domain,\n",
        "    i.e. the voltage value over time correlates closely to the\n",
        "    cursor movement over the same time period.\n",
        "    :param np.ndarray binned_V: Binned voltage data,\n",
        "    [num_bins x binsize x num_channels].\n",
        "    :returns np.ndarray lmp: LMP of dimension [num_bins x channels].\n",
        "    \"\"\"\n",
        "    # simply compute the voltage average over bins\n",
        "    lmp = np.mean(binned_V, axis=1)\n",
        "    return lmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9Qj9Fs0Wd-Y"
      },
      "outputs": [],
      "source": [
        "# some channels might show synchronization with local motor potential\n",
        "patient = alldat[0][0]\n",
        "label_to_bin = ['V']\n",
        "tobin = dict([ (key, patient[key]) for key in label_to_bin])\n",
        "V_binned = bin_data_to_binsize(tobin, binsize=200, stepsize=40, show_conversion=True)\n",
        "\n",
        "V_binned = V_binned['V_binned']\n",
        "V_cm_avg_ref = common_average_reference(V_data=V_binned)\n",
        "\n",
        "lmp = extract_lmp_from_binned_voltage(binned_V=V_cm_avg_ref)\n",
        "\n",
        "# plot all channels under the actual cursor movements\n",
        "num_channels = lmp.shape[1]\n",
        "FIGSIZE_X, FIGSIZE_Y = 7, max(7, (num_channels + 2) // 2)\n",
        "YTICK_FONTSIZE = 7.5\n",
        "\n",
        "fig, axs = plt.subplots(num_channels + 2, 1, figsize=(FIGSIZE_X, FIGSIZE_Y))\n",
        "# plot the visualizations\n",
        "for channel_idx in range(num_channels + 2):\n",
        "    if channel_idx == 0:\n",
        "        axs[0].plot(patient['cursorX'][::40])\n",
        "        axs[0].set_ylabel('X')\n",
        "    elif channel_idx == 1:\n",
        "        axs[1].plot(patient['cursorY'][::40])\n",
        "        axs[1].set_ylabel('Y')\n",
        "    else:\n",
        "        axs[channel_idx].plot(lmp[:, channel_idx - 2])\n",
        "        axs[channel_idx].set_ylabel(channel_idx)\n",
        "    axs[channel_idx].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "    axs[channel_idx].tick_params(axis='y', which='both', labelsize=YTICK_FONTSIZE)\n",
        "    # add a label indicating which channel to each plot\n",
        "plt.tight_layout()\n",
        "plt.suptitle(f\"Example LMP from channels over time\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3sboUBiKCgU"
      },
      "source": [
        "### Hjorth Activity, Mobility & Complexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEmjeB3UzR0v"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: 2024/03/20\n",
        "# Last Updated: 2024/03/21\n",
        "\n",
        "def extract_Hjorth_features_from_binned_voltage(binned_V : np.ndarray):\n",
        "    \"\"\"\n",
        "    Extracts the Hjorth activity, mobility and complexity features\n",
        "    from binned voltage data.\n",
        "    - Hjorth activity: variance\n",
        "    - Mobility: mean frequency\n",
        "    - Complexity: changes in frequency over a given time period\n",
        "    :param np.ndarray binned_V: Binned voltage data,\n",
        "    [num_bins x binsize x num_channels].\n",
        "    :returns np.ndarray activity, mobility, complexity: Features, each of\n",
        "    dimension [num_bins x channels].\n",
        "    \"\"\"\n",
        "\n",
        "    activity = extract_Hjorth_activity_from_binned_voltage(binned_V)\n",
        "    mobility = extract_Hjorth_mobility_from_binned_voltage(binned_V)\n",
        "    complexity = extract_Hjorth_complexity_from_binned_voltage(binned_V)\n",
        "\n",
        "    return activity, mobility, complexity\n",
        "\n",
        "def extract_Hjorth_activity_from_binned_voltage(binned_V : np.ndarray):\n",
        "    \"\"\"\n",
        "    Extracts the Hjorth activity features from binned voltage data.\n",
        "    Hjorth activity is the variance of signal amplitude per bin.\n",
        "    :param np.ndarray binned_V: Binned voltage data,\n",
        "    [num_bins x binsize x num_channels].\n",
        "    :returns np.ndarray activity: Feature of dim [num_bins x channels].\n",
        "    \"\"\"\n",
        "    # simply return the variance per bins\n",
        "    activity = np.var(binned_V, axis=1)\n",
        "    return activity\n",
        "\n",
        "def extract_Hjorth_mobility_from_binned_voltage(binned_V : np.ndarray):\n",
        "    \"\"\"\n",
        "    Extracts the Hjorth mobility features from binned voltage data.\n",
        "    Hjorth mobility is computed as the square root of the ratio of\n",
        "      1 - the variance of the first derivative of voltage over time\n",
        "      2 - the variance of voltage amplitude\n",
        "    :param np.ndarray binned_V: Binned voltage data,\n",
        "    [num_bins x binsize x num_channels].\n",
        "    :returns np.ndarray mobility: Feature of dim [num_bins x channels].\n",
        "    \"\"\"\n",
        "    if binned_V.shape[1] < 2:\n",
        "      raise Exception(\"Hjorth mobility can be ill-defined when \" +\n",
        "                    \"the bin size is smaller than 2...\")\n",
        "\n",
        "    voltage_derivative_over_time = np.diff(binned_V, axis=1)\n",
        "\n",
        "    V_derivative_variance = np.var(voltage_derivative_over_time, axis=1)\n",
        "    V_amplitude_variance = np.var(binned_V, axis=1)\n",
        "\n",
        "    mobility_sq = np.divide(V_derivative_variance, V_amplitude_variance)\n",
        "    mobility = np.sqrt(mobility_sq)\n",
        "    return mobility\n",
        "\n",
        "def extract_Hjorth_complexity_from_binned_voltage(binned_V : np.ndarray):\n",
        "    \"\"\"\n",
        "    Extracts the Hjorth complexity features from binned voltage data.\n",
        "    Hjorth complexity is computed as the ratio of:\n",
        "      1 - Hjorth mobility but computed with variance of the first derivative\n",
        "          of the amplitude signal vs. the second derivative of the signal\n",
        "      2 - Hjorth mobility, computed with amplitude below its first derivative\n",
        "          with respect to time\n",
        "      :param np.ndarray binned_V: Binned voltage data,\n",
        "    [num_bins x binsize x num_channels].\n",
        "    :returns np.ndarray complexity: Feature of dim [num_bins x channels].\n",
        "    \"\"\"\n",
        "    if binned_V.shape[1] < 3:\n",
        "      raise Exception(\"Hjorth complexity can be ill-defined when \" +\n",
        "                    \"the bin size is smaller than 3...\")\n",
        "    voltage_derivative_over_time = np.diff(binned_V, axis=1)\n",
        "    voltage_2nd_derivative_over_time = np.diff(voltage_derivative_over_time, axis=1)\n",
        "    # compute usual mobility first\n",
        "    V_derivative_variance = np.var(voltage_derivative_over_time, axis=1)\n",
        "    V_amplitude_variance = np.var(binned_V, axis=1)\n",
        "    mobility_sq = np.divide(V_derivative_variance, V_amplitude_variance)\n",
        "    mobility = np.sqrt(mobility_sq)\n",
        "    # compute first derivative-based mobility next\n",
        "    V_2nd_derivative_variance = np.var(voltage_2nd_derivative_over_time, axis=1)\n",
        "    first_derivative_mobility_sq = np.divide(V_2nd_derivative_variance, V_derivative_variance)\n",
        "    first_derivative_mobility = np.sqrt(first_derivative_mobility_sq)\n",
        "    # finally compute the ratio\n",
        "    complexity = np.divide(first_derivative_mobility, mobility)\n",
        "    return complexity\n",
        "\n",
        "def graph_Hjorth_features_from_binned_voltage_data(original_V : np.ndarray,\n",
        "                                                   binned_V : np.ndarray):\n",
        "    \"\"\"\n",
        "    Extracts, then graphs the activity, mobility and complexity features\n",
        "    from the given data.\n",
        "    :param np.ndarray original_V: The original voltage data, dimension has to be\n",
        "    [timepoints,], that is a single channel.\n",
        "    :param np.ndarray binned_V: The binned data we extract features from;\n",
        "    has to be of dimension [num_bins x binsize], that is a single channel.\n",
        "    \"\"\"\n",
        "    def normalize_numpy_array(arr : np.ndarray):\n",
        "         return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
        "\n",
        "    norm_signal = normalize_numpy_array(original_V)\n",
        "    original_to_binned_ratio = norm_signal.shape[0]/binned_V.shape[0]\n",
        "    norm_signal_x = [val/original_to_binned_ratio for val in range(len(norm_signal))] # align ticks\n",
        "    # extract the different features\n",
        "    activity, mobility, complexity = extract_Hjorth_features_from_binned_voltage(binned_V=binned_V)\n",
        "    first_derivative_signal = np.diff(binned_V, axis=1)\n",
        "    _, second_mobility, _ = extract_Hjorth_features_from_binned_voltage(binned_V=first_derivative_signal)\n",
        "    # create 3 plots:\n",
        "    # 1 - visualizes the activity parameter with the original signal\n",
        "    _, ax1 = plt.subplots()\n",
        "    norm_activity = normalize_numpy_array(activity)\n",
        "    ax1.plot(norm_signal_x, norm_signal,   label='Normalized signal', color='b')\n",
        "    ax1.plot(norm_activity, label='Normalized activity', color='g')\n",
        "    plt.title(\"Activity = variance of binned original signal (Normalized)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # 2 - visualizes the mobility parameter with the original signal,\n",
        "    # its first derivative and the activity\n",
        "    _, ax2 = plt.subplots()\n",
        "    norm_mobility = normalize_numpy_array(mobility)\n",
        "    norm_signal_1st_derivative = normalize_numpy_array(np.diff(norm_signal))\n",
        "    ax2.plot(norm_signal_x, norm_signal,   label='Normalized signal', color='b')\n",
        "    ax2.plot(norm_signal_x[:-1], norm_signal_1st_derivative,\n",
        "             label='Normalized first derivative', color='cyan')\n",
        "    ax2.plot(norm_activity, label='Normalized activity', color='g')\n",
        "    ax2.plot(norm_mobility, label='Normalized mobility', color='r')\n",
        "    plt.title(\"Mobility = sqrt of [variance of first derivative of signal / activity] (Normalized)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # 3 - visualizes the complexity parameter with the original signal,\n",
        "    # its mobility and 2nd derivative\n",
        "    _, ax3 = plt.subplots()\n",
        "    norm_complexity = normalize_numpy_array(complexity)\n",
        "    norm_signal_2nd_derivative = normalize_numpy_array(np.diff(norm_signal_1st_derivative))\n",
        "    norm_2nd_mobility = normalize_numpy_array(second_mobility)\n",
        "    ax3.plot(norm_signal_x, norm_signal, label='Normalized signal', color='b')\n",
        "    ax3.plot(norm_mobility, label='Normalized mobility', color='r')\n",
        "    ax3.plot(norm_signal_x[:-2], norm_signal_2nd_derivative,\n",
        "             label='Normalized second derivative', color='purple')\n",
        "    ax3.plot(norm_2nd_mobility, label='Normalized first derivative mobility', color='pink')\n",
        "    ax3.plot(norm_complexity, label='Normalized complexity', color='brown')\n",
        "    plt.title(\"Complexity = ratio of [mobility computed on first derivative of signal / mobility of signal] (Normalized)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def sanity_check_extract_Hjorth_features_from_binned_voltage():\n",
        "    # we first establish a data that is a sine wave from 0 to 6pi\n",
        "    cycles = 2\n",
        "    sine_wave = np.sin(np.linspace(0, cycles*2*np.pi, num=cycles*SAMPLE_RATE*10))\n",
        "    # bin the wave into a comparable format to ours\n",
        "    tobin = {'sine_wave' : sine_wave}\n",
        "    wave_binned = bin_data_to_binsize(tobin, binsize=200, stepsize=40, show_conversion=True)\n",
        "    wave_binned = wave_binned['sine_wave_binned']\n",
        "    # plot the different features\n",
        "    graph_Hjorth_features_from_binned_voltage_data(original_V=sine_wave,\n",
        "                                                   binned_V=wave_binned)\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "        Looks good to me?\n",
        "        1 - Activity is the variance of the sine wave, which roughly correlates\n",
        "             with the absolute value of the derivative of the wave? Since in the\n",
        "             local region, a bigger change within the region leads to higher\n",
        "             variance.\n",
        "        2 - Mobility is the ratio of variance of first derivative / activity.\n",
        "            The variance of first derivative of sine wave is that of the cosine\n",
        "            wave, which is essentially the same shape but shifted by sine wave cycle/4.\n",
        "            If we consider how the ratio should behave, the peaks and troughs seem to match.\n",
        "        3 - Complexity is the ratio of mobility computed using the first derivative,\n",
        "            and mobility computed above.\n",
        "            Mobility computed using the first derivative would be mobility, but\n",
        "            when the original wave is a cosine wave - which is sine-wave-cycle/4 shifted\n",
        "            compared to the sine wave - it's mobility signal is also shifted by\n",
        "            sine-wave-cycle/4. Considering this shift, the peaks and troughs for\n",
        "            this ratio seem to make sense.\n",
        "        \"\"\")\n",
        "\n",
        "sanity_check_extract_Hjorth_features_from_binned_voltage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIbVA2q-38w2"
      },
      "outputs": [],
      "source": [
        "patient = alldat[0][0]\n",
        "label_to_bin = ['V']\n",
        "tobin = dict([ (key, patient[key]) for key in label_to_bin])\n",
        "BINSIZE, STEPSIZE = 200, 40\n",
        "V_binned = bin_data_to_binsize(tobin, binsize=BINSIZE, stepsize=STEPSIZE, show_conversion=True)\n",
        "\n",
        "V_binned = V_binned['V_binned']\n",
        "V_cm_avg_ref = common_average_reference(V_data=V_binned)\n",
        "\n",
        "unbinned_V_cm_avg_ref = common_average_reference(V_data=patient['V'])\n",
        "\n",
        "START, STOP = 0, 10\n",
        "CHANNEL_N = 0\n",
        "print(f\"- For bins {START} to {STOP} in channel {CHANNEL_N}: \")\n",
        "graph_Hjorth_features_from_binned_voltage_data(\n",
        "    original_V=unbinned_V_cm_avg_ref[START*STEPSIZE : STOP*STEPSIZE, CHANNEL_N],\n",
        "    binned_V=V_cm_avg_ref[START : STOP, : , CHANNEL_N])\n",
        "\n",
        "# we can also visualize them one by one - but it is hard to see if it makes sense\n",
        "# activity, mobility, complexity = extract_Hjorth_features_from_binned_voltage(binned_V=V_cm_avg_ref)\n",
        "\n",
        "# for feature, feat_name in zip([activity, mobility, complexity],\n",
        "#                               [\"activity\", \"mobility\", \"complexity\"]):\n",
        "#   plt.plot(feature[:, 0])\n",
        "#   plt.title(f\"Example {feat_name} from channel 0 over time\")\n",
        "#   plt.grid()\n",
        "#   plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHY9YvwkCMEL"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: 2024/03/27\n",
        "# Last updated: 2024/03/27\n",
        "\n",
        "FREQUENCY_RANGES = ( (8,13),\n",
        "                     (13,30),\n",
        "                     (30,60),\n",
        "                     (60,100),\n",
        "                     (100,200) )\n",
        "\n",
        "def extract_all_features_from_binned_voltage(\n",
        "    binned_V : np.ndarray,\n",
        "    frequency_ranges : Tuple[Tuple[int]]=FREQUENCY_RANGES\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Extract all of: frequency features, LMP, Hjorth features,\n",
        "    from a given binned voltage data that is ready for extraction.\n",
        "    If given frequency_ranges, extract the corresponding frequency\n",
        "    features - otherwise, defaults to FREQUENCY_RANGES.\n",
        "    :param np.ndarray binned_V: A binned voltage data array, of shape\n",
        "    [binnumber x binsize x channelsize].\n",
        "    :param Tuple[Tuple[int]] frequency_ranges: A tuple of tuple of integer\n",
        "    expressing the frequency ranges as ((low, high), (low, high), ...) etc.\n",
        "    Corresponding frequency ranges will be extracted, where the size of frequency\n",
        "    bins is determined by the bin size N of voltage data (N/2, if I understand correctly?)\n",
        "    :returns np.ndarray features: A numpy array of all features stacked together,\n",
        "    in order: 1) frequency ranges, in the given order. 2) LMP.\n",
        "    3) Hjorth activity, then mobility, then complexity.\n",
        "    Of shape [(num_ranges + 4) x num_bins x num_channels].\n",
        "    \"\"\"\n",
        "    frequency_features, exact_freq_ranges = extract_frequency_features_from_binned_voltage(\n",
        "        binned_V, frequency_ranges)\n",
        "    lmp = extract_lmp_from_binned_voltage(binned_V)\n",
        "    activity, mobility, complexity = extract_Hjorth_features_from_binned_voltage(\n",
        "        binned_V\n",
        "    )\n",
        "    # stack results on top of each other in specified order\n",
        "    features = np.stack(\n",
        "        frequency_features + [lmp, activity, mobility, complexity]\n",
        "    )\n",
        "    return features, exact_freq_ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMmByK9tDJP5"
      },
      "outputs": [],
      "source": [
        "patient = alldat[0][0]\n",
        "label_to_bin = ['V']\n",
        "tobin = dict([ (key, patient[key]) for key in label_to_bin])\n",
        "V_binned = bin_data_to_binsize(tobin, binsize=200, stepsize=40, show_conversion=True)\n",
        "\n",
        "V_binned = V_binned['V_binned']\n",
        "V_cm_avg_ref = common_average_reference(V_data=V_binned)\n",
        "\n",
        "features, exact_freq_ranges = extract_all_features_from_binned_voltage(binned_V=V_cm_avg_ref)\n",
        "print(f\"features.shape: {features.shape}\")\n",
        "print(f\"Exact frequency ranges: {exact_freq_ranges}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaK2AaQHm2Ut"
      },
      "source": [
        "## Feature Selection\n",
        "---\n",
        "Feature extraction produces **[9 features x ECoG channel x epoch]**.\n",
        "\n",
        "We now **select a subset of features** that exhibit **high correlation** with the **joystick label within that epoch**.\n",
        "\n",
        "Selection is made so that we **train Gradient Boosted Trees**, seeing **what proportion of the best correlated features** we need before **achieving a set accuracy / tapering off** (let's say we aim for ~**90% accuracy**).\n",
        "\n",
        "<sub>This avoids overfitting by ML algorithms to the training data.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's explore both **inter-patient prediction** and **cross-patient prediction capability** by, for example:\n",
        "- Selecting a <b>subset of features with <u>agreed highest correlation</u> between patient 1~2</b>, and observing:\n",
        " - prediction within patients 1~2 with <b>k-fold cross-validation</b>.\n",
        " - generalization to other patients through <b>prediction to patients 3~4</b>.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "What we do:\n",
        "\n",
        "Compute the **Pearson correlation coefficients** between the **joystick labels in the epoch** and the **9 produced features**.\n",
        "\n",
        "We take a **One-vs-rest approach**:\n",
        "1. We first create a **dataset where each quadrant in turn is relabeled as 1**, and **the other quadrants are labeled as 0**\n",
        "\n",
        "2. The **correlation is then computed**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson Correlation Coefficient Calculation"
      ],
      "metadata": {
        "id": "4H9-jli11myv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYuY--Jxuomb"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: 2024/03/24\n",
        "# Last Updated: 2024/03/24\n",
        "\n",
        "def create_dataset_with_positive_label_class(labeled_dataset : np.ndarray,\n",
        "                                             positive_label : np.ndarray):\n",
        "    \"\"\"\n",
        "    Creates a dataset where the classes given as 'positive_label' are relabeled\n",
        "    to 1, and all other classes are relabeled to 0. This can then be used to\n",
        "    compute the pearson correlation coefficient with features.\n",
        "    :param np.ndarray labeled_dataset: The dataset which contains labels.\n",
        "    :param np.ndarray positive_label: A list of the values that are relabeled\n",
        "    as positive cases (1). All other values are relabeled as negative cases (0).\n",
        "    \"\"\"\n",
        "    # if positive_label is empty, raise an error\n",
        "    if positive_label.size == 0:\n",
        "        raise Exception(\"positive_label is empty, which is unexpected...\")\n",
        "\n",
        "    # first check if labeled_dataset and positive_label hold the same data type\n",
        "    if labeled_dataset.dtype != positive_label.dtype:\n",
        "        # if not, try casting the former to that of the latter\n",
        "        if np.can_cast(positive_label.dtype, labeled_dataset.dtype):\n",
        "            positive_label = positive_label.astype(labeled_dataset.dtype)\n",
        "        else:\n",
        "            raise Exception(\"positive_label has to hold datatype identical, \" +\n",
        "                            \"or castable to that of labeled_dataset...\")\n",
        "\n",
        "    # create a negative-case array of same array as labeled dataset\n",
        "    returned = np.zeros_like(labeled_dataset)\n",
        "    # then we relabel positive cases\n",
        "    pos_label_loc = np.isin(element=labeled_dataset, test_elements=positive_label)\n",
        "    returned[pos_label_loc] = 1\n",
        "\n",
        "    return returned\n",
        "\n",
        "def sanity_check_create_dataset_with_positive_label_class():\n",
        "    labeled_dataset = np.array([[0,0,1],[2,0,1],[2,99,2],[1,99,0]])\n",
        "    positive_label1 = np.array([0])\n",
        "    positive_label2 = np.array([0, 1])\n",
        "    positive_label3 = np.array([2,99])\n",
        "    # for each case, the expected is as follows:\n",
        "    expected_relabeled1 = np.array([[1,1,0],[0,1,0],[0,0,0],[0,0,1]])\n",
        "    expected_relabeled2 = np.array([[1,1,1],[0,1,1],[0,0,0],[1,0,1]])\n",
        "    expected_relabeled3 = np.array([[0,0,0],[1,0,0],[1,1,1],[0,1,0]])\n",
        "    # compute and compare\n",
        "    actual_relabeled1 = create_dataset_with_positive_label_class(labeled_dataset, positive_label1)\n",
        "    actual_relabeled2 = create_dataset_with_positive_label_class(labeled_dataset, positive_label2)\n",
        "    actual_relabeled3 = create_dataset_with_positive_label_class(labeled_dataset, positive_label3)\n",
        "\n",
        "    assert np.array_equal(expected_relabeled1, actual_relabeled1)\n",
        "    assert np.array_equal(expected_relabeled2, actual_relabeled2)\n",
        "    assert np.array_equal(expected_relabeled3, actual_relabeled3)\n",
        "\n",
        "sanity_check_create_dataset_with_positive_label_class()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCEvSopL525z"
      },
      "outputs": [],
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: 2024/03/24\n",
        "# Last Updated: 2024/03/27\n",
        "\n",
        "def compute_PCC_and_choose_top_n_features_per_class(\n",
        "    features : np.ndarray,\n",
        "    labels : np.ndarray,\n",
        "    n: int\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Computes the Pearson Correlation Coefficient (PCC) between each feature\n",
        "    and the labeled dataset in a One-vs-rest fashion. Then, for each label\n",
        "    class, extract the top n correlated features to that label.\n",
        "    :param np.ndarray features: The features, put into shape of\n",
        "    [num_features x num_bins x channels].\n",
        "    :param np.ndarray labels: The corresponding labels to those feature bins,\n",
        "    of shape [num_bins, ].\n",
        "    :param int n: The number of top feature we extract in terms of correlation\n",
        "    to each of the unique label classes found in 'labels'.\n",
        "    \"\"\"\n",
        "    if features.shape[1] != labels.shape[0]:\n",
        "       raise Exception(\"2nd dimension of features and 1st dimension of labels \" +\n",
        "                       \"have to have the same size, but had instead \" +\n",
        "                       f\"features: {features.shape[1]} vs. labels: {labels.shape[0]}...\")\n",
        "\n",
        "    # first extract unique labels and corresponding one-vs-rest datasets,\n",
        "    # stacking them on top of each other to pass into np.corrcoef\n",
        "    unique_labels = np.unique(labels).tolist()\n",
        "    unique_labels.remove(INCONSISTENCY_EXPRESSING_VALUE)\n",
        "    datasets = [create_dataset_with_positive_label_class(labels, np.array(unique_l))\n",
        "                for unique_l in unique_labels]\n",
        "    datasets = np.array(datasets)\n",
        "\n",
        "    def compute_correlation_for_each(features, datasets, unique_labels):\n",
        "        \"\"\"\n",
        "        Computes correlation while extracting individual features & dataset.\n",
        "        :returns Dict: A dictionary mapping the unique labels to their\n",
        "        n highest-correlated features, together with their correlations in form\n",
        "        [(5th_high_corr_feature_idx, channel_idx, correlation), ...\n",
        "         (highest_corr_feature_idx,  channel_idx, correlation)]\n",
        "        \"\"\"\n",
        "        def flat_idx_to_feature_times_channel_idx(idx : int):\n",
        "            \"\"\"Convert index in a flat array to that of feature x channel.\"\"\"\n",
        "            num_features, num_channels = features.shape[0], features.shape[2]\n",
        "            row = idx // num_channels\n",
        "            col = idx - row * num_channels\n",
        "            return row, col\n",
        "\n",
        "        returned = {}\n",
        "        # for each dataset\n",
        "        for dataset_idx in range(datasets.shape[0]):\n",
        "            ds = datasets[dataset_idx, :]\n",
        "            unique_l = unique_labels[dataset_idx]\n",
        "            all_correlations = []\n",
        "            #  for each feature x channel combination:\n",
        "            for feature_idx in range(features.shape[0]):\n",
        "                for channel_idx in range(features.shape[2]):\n",
        "                    single_feature = features[feature_idx, :, channel_idx]\n",
        "                    # compute the correlation and store it\n",
        "                    correlation = np.corrcoef(single_feature, ds)\n",
        "                    all_correlations.append(correlation[0, 1].item())\n",
        "            # for the given dataset, determine the top n correlated features\n",
        "            all_correlations = np.array(all_correlations)\n",
        "            correlations_abs = np.abs(all_correlations)\n",
        "            # obtain ind, the index of n highest elements in the list\n",
        "            local_n = min(n, features.shape[0]*features.shape[2]) # if n too big, truncate\n",
        "            ind = np.argpartition(correlations_abs, -local_n)[-local_n:]\n",
        "            ind = ind[np.argsort(correlations_abs[ind])].tolist() #sort result\n",
        "            # convert it into a [feature_number, channel_number, correlation] format\n",
        "            converted_ind = []\n",
        "            for idx in ind:\n",
        "                correlation = all_correlations[idx].item()\n",
        "                newly_formatted = flat_idx_to_feature_times_channel_idx(idx) + \\\n",
        "                                  (correlation,)\n",
        "                converted_ind.append(newly_formatted)\n",
        "\n",
        "            returned[unique_l] = converted_ind\n",
        "        return returned\n",
        "\n",
        "    top_n_features = compute_correlation_for_each(features, datasets, unique_labels)\n",
        "\n",
        "    return top_n_features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: 2024/04/04\n",
        "# Last Updated: 2024/04/04\n",
        "\n",
        "def plot_features_to_label_scatter_graph(features : np.ndarray,\n",
        "                                         label : np.ndarray):\n",
        "    \"\"\"\n",
        "    Plots a scatter plot from the given features & label.\n",
        "    The last dimension of features, and that of label have to be the same size.\n",
        "    :param np.ndarray features: An array of feature, shaped\n",
        "    [num_features x num_channels x num_bins].\n",
        "    :param np.ndarray label: An array of label, shaped [num_bins, ].\n",
        "    \"\"\"\n",
        "    if len(features.shape) != 3:\n",
        "        raise Exception(\"features has to have dimensions \" +\n",
        "                        \"[num_features x num_channels x num_bins] ...\")\n",
        "    elif len(label.shape) != 1:\n",
        "        raise Exception(\"label has to have a single dimension...\")\n",
        "    elif features.shape[-1] != label.shape[-1]:\n",
        "        raise Exception(\"features's last dimension and label have to be the same shape...\")\n",
        "    # plot a graph for each feature vs. the given label\n",
        "    num_features, num_channels = features.shape[0], features.shape[1]\n",
        "    # each column is a feature, each row a channel\n",
        "    for feat_idx in range(num_features):\n",
        "        _, axes = plt.subplots(num_channels, 1, figsize=(6,64))\n",
        "        for channel_idx in range(num_channels):\n",
        "            ax = axes[channel_idx]\n",
        "            feat = features[feat_idx, channel_idx, :]\n",
        "            ax.scatter(label, feat)\n",
        "            ax.set_ylabel(channel_idx)\n",
        "            ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "        plt.suptitle(f\"Feature {feat_idx} vs. Label\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "EaRtN6W4nrc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdDo-e88B4Il"
      },
      "outputs": [],
      "source": [
        "BINSIZE, STEPSIZE = 200, 40\n",
        "\n",
        "patient = alldat[0][0]\n",
        "\n",
        "# extract feature - it takes a long time, so check if it is already computed\n",
        "try:\n",
        "    print(f\"features.shape :{features.shape}\")\n",
        "except NameError:\n",
        "    label_to_bin = ['V']\n",
        "    tobin = dict([ (key, patient[key]) for key in label_to_bin])\n",
        "    V_binned = bin_data_to_binsize(tobin, binsize=BINSIZE, stepsize=STEPSIZE, show_conversion=True)\n",
        "\n",
        "    V_binned = V_binned['V_binned']\n",
        "    V_cm_avg_ref = common_average_reference(V_data=V_binned)\n",
        "    features, _ = extract_all_features_from_binned_voltage(binned_V=V_cm_avg_ref)\n",
        "    print(f\"features.shape :{features.shape}\")\n",
        "\n",
        "labels, consistent_percentage = label_data_by_quadrant(\n",
        "    patient=patient, binwidth=BINSIZE, stepwidth=STEPSIZE)\n",
        "\n",
        "print(f\"The percentage of consistent labels obtained with binning was: {consistent_percentage}%.\")\n",
        "\n",
        "# ***AN IMPORTANT DECISION HERE!***\n",
        "# LABELS ARE EXTRACTED FROM DERIVATIVE WITH RESPECT TO TIME, RESULTING\n",
        "# IN ONE LESS BIN OF LABEL COMPARED TO BINS OF FEATURES.\n",
        "# FROM NOW ON, WE ASSUME THAT:\n",
        "#  - THE iTH FEATURE BIN CORRESPONDS TO THE iTH LABEL BIN\n",
        "#  -> ONLY THE LAST FEATURE BIN LACKS A COUNTERPART LABEL\n",
        "\n",
        "truncated_features = features[:, :-1, :] # match the length of the labels\n",
        "\n",
        "top_n_features = compute_PCC_and_choose_top_n_features_per_class(\n",
        "    features=truncated_features,\n",
        "    labels=labels,\n",
        "    n=999\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizes a scatter plot of label vs. feature to visually inspect correlation\n",
        "label_group_1 = create_dataset_with_positive_label_class(\n",
        "    labeled_dataset=labels,\n",
        "    positive_label=np.array([1])\n",
        "    )\n",
        "\n",
        "# for channel_idx in range(truncated_features.shape[2]):\n",
        "#     feature_channel = truncated_features[0,:,channel_idx]\n",
        "#     plot_features_to_label_scatter_graph(features=feature_channel,\n",
        "#                                          label=label_group_1)\n",
        "axes_swapped_features = np.swapaxes(truncated_features, 1, 2)\n",
        "if False:\n",
        "    plot_features_to_label_scatter_graph(features=axes_swapped_features,\n",
        "                                        label=label_group_1)"
      ],
      "metadata": {
        "id": "cXNJf75eKWId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Selection Using Gradient Boosted Tree Accuracy\n",
        "\n",
        "Again, as [**Lin Yao et al (2022)**](https://iopscience.iop.org/article/10.1088/1741-2552/ac4ed1/pdf) did, we:\n",
        "1. **<u>Selected features</u>** to pass to machine learning algorithms in **descending order** of **squared PCC** value.\n",
        "2. **<u>Train a Gradient Boosted Tree</u>** using the growing set of features, evaluated through **k-fold cross validation**. *DO WE NEED CHRONOLOGICAL CV HERE?\n",
        "3. **Observe the number of features** required for the **<u>accuracy to taper off</u>**.\n",
        "\n",
        "We use **LightGBM** as implementation of the gradient boosted trees."
      ],
      "metadata": {
        "id": "KVq0gcgH1sl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Author: Akira Kudo\n",
        "# Created: 2024/04/06\n",
        "# Last updated: 2024/04/06\n",
        "\n",
        "def temporal_contatenation_of_features(features : np.ndarray, past_n : int):\n",
        "    \"\"\"\n",
        "    Recreate a set of feature bins which have been enhanced by concatenating\n",
        "    every 'past_n' bins together.\n",
        "    e.g. [0,1,2,3,4,5,6,7] with past_n = 3 becomes:\n",
        "         [[0,1,2],[2,3,4],[3,4,5],[4,5,6],[5,6,7]].\n",
        "    :param np.ndarray features: A list of features of shape\n",
        "    [num_features x num_bins x num_channels].\n",
        "    :param int past_n: The number of features to concatenate towards the past.\n",
        "    :returns np.ndarray concatenated: Concatenated features of shape,\n",
        "    [(num_bins - past_n + 1) x num_features x past_n x num_channels].\n",
        "    \"\"\"\n",
        "    if features.shape[1] < past_n:\n",
        "        raise Exception(\"The temporal dimension of features must be bigger than \" +\n",
        "                        f\"past_n, {past_n}...\")\n",
        "\n",
        "    concatenated_bins = []\n",
        "    for bin_idx in range(past_n - 1, features.shape[1]):\n",
        "        concat_start, concat_end = bin_idx - past_n + 1, bin_idx + 1\n",
        "        conc_bin = features[:,concat_start:concat_end,:]\n",
        "        concatenated_bins.append(conc_bin)\n",
        "    concatenated_bins = np.array(concatenated_bins)\n",
        "    return concatenated_bins\n",
        "\n",
        "def sanity_check_temporal_concatenation_of_features():\n",
        "    # num_features = 2, num_bins = 8, num_channels = 3\n",
        "    data = np.array([[[0,10,20],\n",
        "                      [1,11,21],\n",
        "                      [2,12,22],\n",
        "                      [3,13,23],\n",
        "                      [4,14,24],\n",
        "                      [5,15,25],\n",
        "                      [6,16,26],\n",
        "                      [7,17,27]],\n",
        "                     [[100,110,120],\n",
        "                      [101,111,121],\n",
        "                      [102,112,122],\n",
        "                      [103,113,123],\n",
        "                      [104,114,124],\n",
        "                      [105,115,125],\n",
        "                      [106,116,126],\n",
        "                      [107,117,127]]])\n",
        "    print(f\"data.shape: {data.shape}\")\n",
        "    # then we bin this with past_n = 3\n",
        "    actual = temporal_contatenation_of_features(features=data, past_n=3)\n",
        "\n",
        "    expected = np.array([[[[0,10,20],[1,11,21],[2,12,22]],\n",
        "                          [[100,110,120],[101,111,121],[102,112,122]]],\n",
        "                         [[[1,11,21],[2,12,22],[3,13,23]],\n",
        "                          [[101,111,121],[102,112,122],[103,113,123]]],\n",
        "                         [[[2,12,22],[3,13,23],[4,14,24]],\n",
        "                          [[102,112,122],[103,113,123],[104,114,124]]],\n",
        "                         [[[3,13,23],[4,14,24],[5,15,25]],\n",
        "                          [[103,113,123],[104,114,124],[105,115,125]]],\n",
        "                         [[[4,14,24],[5,15,25],[6,16,26]],\n",
        "                          [[104,114,124],[105,115,125],[106,116,126]]],\n",
        "                         [[[5,15,25],[6,16,26],[7,17,27]],\n",
        "                          [[105,115,125],[106,116,126],[107,117,127]]]])\n",
        "    print(f\"actual.shape: {actual.shape}\")\n",
        "    print(f\"expected.shape: {expected.shape}\")\n",
        "    assert np.array_equal(actual, expected)\n",
        "\n",
        "sanity_check_temporal_concatenation_of_features()"
      ],
      "metadata": {
        "id": "Z-PZP4Ab5DXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONCATENATE_N = 21\n",
        "\n",
        "KFOLD = 10\n",
        "SEED = 123\n",
        "\n",
        "# first do temporal concatenation of features over time\n",
        "concatenated_features = temporal_contatenation_of_features(\n",
        "    features=truncated_features, past_n=CONCATENATE_N)\n",
        "# match labels to concatenated_features - leave the first CONCATENATE_N labels out\n",
        "truncated_labels = labels[CONCATENATE_N-1:]\n",
        "# then get train-test splits for k-fold cross validation\n",
        "kf = KFold(n_splits=KFOLD, shuffle=True, random_state=SEED)\n",
        "kf.get_n_splits(truncated_labels)\n",
        "# concatenated_features, truncated_labels,\n",
        "\n",
        "print(splits[0].shape)\n",
        "print(splits[1].shape)\n",
        "print(splits[2].shape)\n",
        "print(splits[3].shape)\n",
        "# feat_train, feat_test, lbl_train, lbl_test =\n",
        "\n",
        "# for i, feat_tr, feat_ts, lbl_tr, lbl_ts in zip(list(range(len(feat_train))), feat_train, feat_test, lbl_train, lbl_test):\n",
        "#     print(f\"Split {i}:\")\n",
        "#     print(f\"Training features shape: {feat_tr.shape}, Testing features shape: {feat_ts.shape}\")\n",
        "#     print(f\"Training num_labels: {lbl_tr.shape}, Testing num_labels: {lbl_ts.shape}\")\n",
        "# train_data = lgb.Dataset(data, label=label)"
      ],
      "metadata": {
        "id": "SysaoeUh1j4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "Updated dataset labeling to also include labeling based on:\n",
        "- average X within bin\n",
        "- average Y within bin\n",
        "- average theta within bin\n",
        "- average r within bin"
      ],
      "metadata": {
        "id": "var6jB8w-ILa"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}